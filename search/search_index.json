{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u6249\u9875 Warning \u7531\u4e8e\u6280\u672f\u9650\u5236\uff0c\u73b0\u5728\u4e2d\u6587\u641c\u7d22\u529f\u80fd\u65e0\u6cd5\u5b8c\u5584\uff0c\u8bf7\u6ce8\u610f\u5f53\u641c\u7d22\u5173\u952e\u8bcd\u7684\u65f6\u5019\u81ea\u884c\u5206\u8bcd\uff0c\u4f8b\u5982\uff1a \u679c\u6728\u9999\u9ec4\u6843\u8461\u5f0f\u86cb\u631e \u65e0\u6cd5\u641c\u5230\u4efb\u4f55\u5185\u5bb9\u3002\u4f46\u53ef\u4ee5\u641c\u7d22 \u679c\u6728\u9999 \u9ec4\u6843 \u8461\u5f0f \u86cb\u631e \u5373\u53ef\u786e\u4fdd\u7d22\u5f15\u5230\u4e0a\u8ff0\u5185\u5bb9\u3002","title":"Home"},{"location":"#_1","text":"Warning \u7531\u4e8e\u6280\u672f\u9650\u5236\uff0c\u73b0\u5728\u4e2d\u6587\u641c\u7d22\u529f\u80fd\u65e0\u6cd5\u5b8c\u5584\uff0c\u8bf7\u6ce8\u610f\u5f53\u641c\u7d22\u5173\u952e\u8bcd\u7684\u65f6\u5019\u81ea\u884c\u5206\u8bcd\uff0c\u4f8b\u5982\uff1a \u679c\u6728\u9999\u9ec4\u6843\u8461\u5f0f\u86cb\u631e \u65e0\u6cd5\u641c\u5230\u4efb\u4f55\u5185\u5bb9\u3002\u4f46\u53ef\u4ee5\u641c\u7d22 \u679c\u6728\u9999 \u9ec4\u6843 \u8461\u5f0f \u86cb\u631e \u5373\u53ef\u786e\u4fdd\u7d22\u5f15\u5230\u4e0a\u8ff0\u5185\u5bb9\u3002","title":"\u6249\u9875"},{"location":"design-pattern/","text":"\u89c2\u5bdf\u8005\u6a21\u5f0f \u89c2\u5bdf\u8005\u6a21\u5f0f\u4e2d\u6709\u4e24\u4e2a\u91cd\u8981\u6982\u5ff5\uff0cObserver \u548c Subject\u3002\u89c2\u5bdf\u8005\u6a21\u5f0f\u7684\u672c\u8d28\u662f Subject \u5bf9\u8c61\u53d1\u751f\u53d8\u5316\u7684\u65f6\u5019\uff0c\u901a\u77e5\u6240\u6709\u89c2\u5bdf\u4e86\u8fd9\u4e2a Subject \u7684 Observer\uff0c\u800c Subject \u4e0d\u9700\u8981 Observer \u5b9e\u9645\u7c7b\u578b\u4fe1\u606f\u3002 class IObserver { public : virtual void OnNotify () = 0 ; }; class ISubject { private : std :: vector < IObserver > observers_ ; public : // \u5229\u7528\u8fd9\u4e2a\u65b9\u6cd5\u53ef\u4ee5\u5c06Observer\u5bf9\u8c61\u6ce8\u518c\u5230Subject\u5bf9\u8c61\u4e2d void RegisterObserver ( IObserver observer ) { observers_ . Add ( observer ); } protected : // \u5f53\u7c7b\u5bf9\u8c61\u72b6\u6001\u53d1\u751f\u6539\u53d8\u65f6\u5019\uff0cNotifyObserver \u65b9\u6cd5\u901a\u77e5\u6240\u6709\u6ce8\u518c\u8fdb\uff08\u8c03\u7528 RegisterObserver\uff09\u8fd9\u4e2a\u7c7b\u5bf9\u8c61\u7684\u89c2\u5bdf\u8005 void NotifyObserver () { foreach ( auto observer : observers_ ) observer . OnNotify (); } }; // \u65b0\u521b\u5efa\u7684\u7c7b\u9700\u8981\u7ee7\u627f\u7684\u57fa\u7c7b class ImportantBaseClass {...}; // \u4f7f\u7528\u591a\u91cd\u7ee7\u627f\u3002\u82e5\u4e0d\u7ee7\u627f IObserve\uff0c\u800c\u5728 SubClass \u6dfb\u52a0 OnNotify \u65b9\u6cd5\uff0c\u5219\u65e0\u6cd5\u4f7f\u7528 Subject \u7684 Register \u65b9\u6cd5 class ObserverSubClass : public ImportantBaseClass , public IObserve {...}; observer class as instance - Software Engineering Stack Exchange \u21a9 Multiple Inheritance: What's a good example? - Stack Overflow \u21a9","title":"\u89c2\u5bdf\u8005\u6a21\u5f0f"},{"location":"design-pattern/#_1","text":"\u89c2\u5bdf\u8005\u6a21\u5f0f\u4e2d\u6709\u4e24\u4e2a\u91cd\u8981\u6982\u5ff5\uff0cObserver \u548c Subject\u3002\u89c2\u5bdf\u8005\u6a21\u5f0f\u7684\u672c\u8d28\u662f Subject \u5bf9\u8c61\u53d1\u751f\u53d8\u5316\u7684\u65f6\u5019\uff0c\u901a\u77e5\u6240\u6709\u89c2\u5bdf\u4e86\u8fd9\u4e2a Subject \u7684 Observer\uff0c\u800c Subject \u4e0d\u9700\u8981 Observer \u5b9e\u9645\u7c7b\u578b\u4fe1\u606f\u3002 class IObserver { public : virtual void OnNotify () = 0 ; }; class ISubject { private : std :: vector < IObserver > observers_ ; public : // \u5229\u7528\u8fd9\u4e2a\u65b9\u6cd5\u53ef\u4ee5\u5c06Observer\u5bf9\u8c61\u6ce8\u518c\u5230Subject\u5bf9\u8c61\u4e2d void RegisterObserver ( IObserver observer ) { observers_ . Add ( observer ); } protected : // \u5f53\u7c7b\u5bf9\u8c61\u72b6\u6001\u53d1\u751f\u6539\u53d8\u65f6\u5019\uff0cNotifyObserver \u65b9\u6cd5\u901a\u77e5\u6240\u6709\u6ce8\u518c\u8fdb\uff08\u8c03\u7528 RegisterObserver\uff09\u8fd9\u4e2a\u7c7b\u5bf9\u8c61\u7684\u89c2\u5bdf\u8005 void NotifyObserver () { foreach ( auto observer : observers_ ) observer . OnNotify (); } }; // \u65b0\u521b\u5efa\u7684\u7c7b\u9700\u8981\u7ee7\u627f\u7684\u57fa\u7c7b class ImportantBaseClass {...}; // \u4f7f\u7528\u591a\u91cd\u7ee7\u627f\u3002\u82e5\u4e0d\u7ee7\u627f IObserve\uff0c\u800c\u5728 SubClass \u6dfb\u52a0 OnNotify \u65b9\u6cd5\uff0c\u5219\u65e0\u6cd5\u4f7f\u7528 Subject \u7684 Register \u65b9\u6cd5 class ObserverSubClass : public ImportantBaseClass , public IObserve {...}; observer class as instance - Software Engineering Stack Exchange \u21a9 Multiple Inheritance: What's a good example? - Stack Overflow \u21a9","title":"\u89c2\u5bdf\u8005\u6a21\u5f0f"},{"location":"github-project-boards/","text":"\u7ec4\u5185\u654f\u6377\u5f00\u53d1\u8bf4\u660e \u5de5\u4f5c\u6d41\u7a0b \u4efb\u52a1\u5212\u5206 Sprint \u5468\u671f\u76ee\u6807 Sprint \u5468\u671f\u4efb\u52a1\u6e05\u5355 \u96be\u5ea6\u6295\u70b9 \u4efb\u52a1\u5206\u914d \u8fdb\u884c\u4efb\u52a1 Github \u521b\u5efa\u65b0\u5206\u652f \u7f16\u7801 TDD \u6d4b\u8bd5\u9a71\u52a8 \u4ee3\u7801\u98ce\u683c\u89c4\u8303 Commit & Push Pull Request Code Review \u6bcf\u5468\u4f9d\u636e\u4efb\u52a1\u6570\u91cf\u8fdb\u884c\u591a\u6b21 Code Review \u5b8c\u6210\u4efb\u52a1 \u6587\u6863\u66f4\u65b0\u5230 Project/docs \u76ee\u5f55\u4e0b \u6d41\u7a0b \u65f6\u95f4\u8bf4\u660e \u65f6\u957f \u4f8b\u4f1a \u5de5\u4f5c\u65e5 \u6bcf\u65e5 10 min \u4efb\u52a1\u5212\u5206 \u5468\u4e00 30 min Code Review \u5468\u4e09\u3001\u5468\u4e94 - \u6587\u6863\u66f4\u65b0 \u5468\u4e94 - \u627e\u8001\u5f20 \u5468\u4e00/\u5468\u4e8c - Github Project Boards Kanban \u67e5\u770b/\u7ba1\u7406\u4efb\u52a1\u8fdb\u5ea6 Kanban \u5361\u7247\u7ba1\u7406 Pull Request \u8fdb\u5ea6 Kanban \u5361\u7247\u7ba1\u7406 Issue \u8fdb\u5ea6 Pull Request \u548c Code Review","title":"\u7ec4\u5185\u654f\u6377\u5f00\u53d1\u8bf4\u660e"},{"location":"github-project-boards/#_1","text":"","title":"\u7ec4\u5185\u654f\u6377\u5f00\u53d1\u8bf4\u660e"},{"location":"github-project-boards/#_2","text":"\u4efb\u52a1\u5212\u5206 Sprint \u5468\u671f\u76ee\u6807 Sprint \u5468\u671f\u4efb\u52a1\u6e05\u5355 \u96be\u5ea6\u6295\u70b9 \u4efb\u52a1\u5206\u914d \u8fdb\u884c\u4efb\u52a1 Github \u521b\u5efa\u65b0\u5206\u652f \u7f16\u7801 TDD \u6d4b\u8bd5\u9a71\u52a8 \u4ee3\u7801\u98ce\u683c\u89c4\u8303 Commit & Push Pull Request Code Review \u6bcf\u5468\u4f9d\u636e\u4efb\u52a1\u6570\u91cf\u8fdb\u884c\u591a\u6b21 Code Review \u5b8c\u6210\u4efb\u52a1 \u6587\u6863\u66f4\u65b0\u5230 Project/docs \u76ee\u5f55\u4e0b \u6d41\u7a0b \u65f6\u95f4\u8bf4\u660e \u65f6\u957f \u4f8b\u4f1a \u5de5\u4f5c\u65e5 \u6bcf\u65e5 10 min \u4efb\u52a1\u5212\u5206 \u5468\u4e00 30 min Code Review \u5468\u4e09\u3001\u5468\u4e94 - \u6587\u6863\u66f4\u65b0 \u5468\u4e94 - \u627e\u8001\u5f20 \u5468\u4e00/\u5468\u4e8c -","title":"\u5de5\u4f5c\u6d41\u7a0b"},{"location":"github-project-boards/#github-project-boards","text":"Kanban \u67e5\u770b/\u7ba1\u7406\u4efb\u52a1\u8fdb\u5ea6 Kanban \u5361\u7247\u7ba1\u7406 Pull Request \u8fdb\u5ea6 Kanban \u5361\u7247\u7ba1\u7406 Issue \u8fdb\u5ea6","title":"Github Project Boards"},{"location":"github-project-boards/#pull-request-code-review","text":"","title":"Pull Request \u548c Code Review"},{"location":"directx/","text":"DirectML \u6458\u8981 Hybrid Rendering \u603b\u7eb2\u3002\u672c\u9875\u5c06\u8bb2\u8ff0DirectML\u539f\u7406\uff0c\u7ed3\u6784\u8bbe\u8ba1\u53ca\u6e32\u67d3\u5b9e\u8df5\u3002\u6e90\u7801\u53ca\u76f8\u5173\u5de5\u5177\uff0c\u53ef\u53c2\u9605 Github \u9879\u76ee\u3002 RTX \u6e32\u67d3\u7ba1\u7ebf\u6982\u8ff0 THE HYBRID RENDERING MODEL Previously, real-time graphics relied on rasterizing triangles to render images. Now, with the introduction of RT Cores and Tensor Cores, Turing hardware enables real-time ray tracing for lighting and the use of AI for image enhancement and other applications. The graphics API has evolved in the same direction, with the introduction of DirectX Raytracing and Windows ML as part of the Windows 10 October 2018 update. Taken together, these changes enable a new rendering model, Hybrid Rendering, in which graphics applications use a combination of traditional rendering, ray traced rendering, and AI to produce amazing images in real time. DirectML \u5b9e\u65f6\u6e32\u67d3 ...domains such as games and engines... For reliable real-time, high-performance, low-latency, and/or resource-constrained scenarios...You can integrate DirectML directly into your existing engine or rendering pipeline. 1 TensorRT/ NGX \u5b9e\u65f6\u6e32\u67d3 NGX DNN models can interface with CUDA 10, the DirectX and Vulkan drivers, as well as take advantage of NVIDIA TensorRT... 2 Introduction to DirectML \u21a9 NVIDIA-Turing-Architecture-Whitepaper \u21a9","title":"\u6249\u9875"},{"location":"directx/#directml","text":"\u6458\u8981 Hybrid Rendering \u603b\u7eb2\u3002\u672c\u9875\u5c06\u8bb2\u8ff0DirectML\u539f\u7406\uff0c\u7ed3\u6784\u8bbe\u8ba1\u53ca\u6e32\u67d3\u5b9e\u8df5\u3002\u6e90\u7801\u53ca\u76f8\u5173\u5de5\u5177\uff0c\u53ef\u53c2\u9605 Github \u9879\u76ee\u3002","title":"DirectML"},{"location":"directx/#rtx","text":"THE HYBRID RENDERING MODEL Previously, real-time graphics relied on rasterizing triangles to render images. Now, with the introduction of RT Cores and Tensor Cores, Turing hardware enables real-time ray tracing for lighting and the use of AI for image enhancement and other applications. The graphics API has evolved in the same direction, with the introduction of DirectX Raytracing and Windows ML as part of the Windows 10 October 2018 update. Taken together, these changes enable a new rendering model, Hybrid Rendering, in which graphics applications use a combination of traditional rendering, ray traced rendering, and AI to produce amazing images in real time.","title":"RTX \u6e32\u67d3\u7ba1\u7ebf\u6982\u8ff0"},{"location":"directx/#directml_1","text":"...domains such as games and engines... For reliable real-time, high-performance, low-latency, and/or resource-constrained scenarios...You can integrate DirectML directly into your existing engine or rendering pipeline. 1","title":"DirectML \u5b9e\u65f6\u6e32\u67d3"},{"location":"directx/#tensorrt-ngx","text":"NGX DNN models can interface with CUDA 10, the DirectX and Vulkan drivers, as well as take advantage of NVIDIA TensorRT... 2 Introduction to DirectML \u21a9 NVIDIA-Turing-Architecture-Whitepaper \u21a9","title":"TensorRT/ NGX \u5b9e\u65f6\u6e32\u67d3"},{"location":"directx/architect/","text":"DirectML \u5de5\u4f5c\u6d41 \u521d\u59cb\u5316 \u521d\u59cb\u5316 Direct3D 12 \u8d44\u6e90 Direct3D 12 device command queue command list descriptor heaps. \u521d\u59cb\u5316 DirectML \u8d44\u6e90 DirectML device Operator instances \u521d\u59cb\u5316 DirectML Operator Operator Initialization \u8f7d\u5165\u6743\u91cd\u8d44\u6e90\uff0c\u7ed1\u5b9a\u5230 initializer \u7684 outputs\uff0c\u7ed1\u5b9a\u5230 compiled \u7684 persistent \u8d44\u6e90 Close and execute your command list. \u6267\u884c \u7ed1\u5b9a Input/ Output Next, you need to bind those Direct3D 12 resources as your input and output tensors. Record into your command list the binding and the execution of your operators. \u6267\u884c DirectML Operator Close and execute your command list.","title":"DirectML \u5de5\u4f5c\u6d41"},{"location":"directx/architect/#directml","text":"","title":"DirectML \u5de5\u4f5c\u6d41"},{"location":"directx/architect/#_1","text":"","title":"\u521d\u59cb\u5316"},{"location":"directx/architect/#direct3d-12","text":"Direct3D 12 device command queue command list descriptor heaps.","title":"\u521d\u59cb\u5316 Direct3D 12 \u8d44\u6e90"},{"location":"directx/architect/#directml_1","text":"DirectML device Operator instances","title":"\u521d\u59cb\u5316 DirectML \u8d44\u6e90"},{"location":"directx/architect/#directml-operator","text":"Operator Initialization \u8f7d\u5165\u6743\u91cd\u8d44\u6e90\uff0c\u7ed1\u5b9a\u5230 initializer \u7684 outputs\uff0c\u7ed1\u5b9a\u5230 compiled \u7684 persistent \u8d44\u6e90 Close and execute your command list.","title":"\u521d\u59cb\u5316 DirectML Operator"},{"location":"directx/architect/#_2","text":"","title":"\u6267\u884c"},{"location":"directx/architect/#input-output","text":"Next, you need to bind those Direct3D 12 resources as your input and output tensors. Record into your command list the binding and the execution of your operators.","title":"\u7ed1\u5b9a Input/ Output"},{"location":"directx/architect/#directml-operator_1","text":"Close and execute your command list.","title":"\u6267\u884c DirectML Operator"},{"location":"directx/binding/","text":"DirectML \u4e2d\u7684\u8d44\u6e90\u7ed1\u5b9a 1 \u5c06\u8d44\u6e90\u7ed1\u5b9a\u5230 pipeline \u4e0a\uff0cDirectML \u8d44\u6e90\u5305\u542b\u4e0b\u9762\u51e0\u79cd Input Output Temporary Persistent \u672c\u6587\u5305\u542b\u7ed1\u5b9a\u7684\u6982\u5ff5\u53ca\u5b9e\u73b0\u7ec6\u8282 \u91cd\u8981\u6982\u5ff5 \u6267\u884c dispatchable\uff08operator initializer/ compiled operator\uff09\uff0c\u9700\u4f9d\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u3002 IDMLDispatchable::GetBindingProperties \u83b7\u53d6\u5f53\u524d dispatchable \u6240\u9700\u7684 descriptors \u6570\u91cf\uff0ctemporary/persistent \u8d44\u6e90 \u521b\u5efa\u8db3\u591f\u5bb9\u7eb3 descriptors \u7684 Direct3D 12 descriptor heap\uff0c\u7ed1\u5b9a\u5230 pipeline IDMLDevice::CreateBindingTable \u521b\u5efa DirectML \u7684 binding table\uff0c\u8868\u793a pipeline \u4e0a\u7684\u8d44\u6e90\u3002\u4f7f\u7528 DML_BINDING_TABLE_DESC \u7ed3\u6784\u6765\u63cf\u8ff0 binding table\u3002 \u521b\u5efa Direct3D 12 buffer \u683c\u5f0f\u7684 temporal/persistent \u8d44\u6e90\uff0c \u7528 DML_BUFFER_BINDING \u548c DML_BINDING_DESC \u63cf\u8ff0\uff0c\u6dfb\u52a0\u5230 binding table \u5f53 dispatchable \u4e3a compiled operator \u65f6\uff0c\u521b\u5efa Direct3D 12 buffer \u8d44\u6e90\uff0c\u6dfb\u52a0\u5230 binding table \u5c06 binding table \u4f5c\u4e3a\u53c2\u6570\u4f20\u5165 IDMLCommandRecorder::RecordDispatch \u83b7\u53d6 Binding Properties Note \u76f8\u540c\u7684 operator \u53ef\u80fd\u4f1a\u6709\u4e0d\u540c\u7684 binding properties DML_BINDING_PROPERTIES \u7ed3\u6784\u5b9a\u4e49\u4e86 dispatchable \u7ed1\u5b9a\u8d44\u6e90\u76f8\u5173\u6570\u636e\uff0c\u4e0b\u9762\u662f\u7ed3\u6784\u7684\u5b8c\u6574\u5b9a\u4e49 struct DML_BINDING_PROPERTIES { UINT RequiredDescriptorCount ; UINT64 TemporaryResourceSize ; UINT64 PersistentResourceSize ; }; \u4f7f\u7528 IDMLDispatchable::GetBindingProperties \u83b7\u53d6\u6570\u636e winrt :: com_ptr <:: IDMLCompiledOperator > dmlCompiledOperator ; // Code to create and compile a DirectML operator goes here. DML_BINDING_PROPERTIES executeDmlBindingProperties { dmlCompiledOperator -> GetBindingProperties () }; winrt :: com_ptr <:: IDMLOperatorInitializer > dmlOperatorInitializer ; // Code to create a DirectML operator initializer goes here. DML_BINDING_PROPERTIES initializeDmlBindingProperties { dmlOperatorInitializer -> GetBindingProperties () }; UINT descriptorCount = ... RequiredDescriptorCount\uff1aDescriptor Heap size TemporaryResourceSize PersistentResourceSize \u521b\u5efa Descriptor Heap DirectML \u5728 Heap \u4e0a\u521b\u5efa\u3001\u7ba1\u7406 Descriptors\u3002 D3D12_DESCRIPTOR_HEAP_DESC \u7ed3\u6784\u63cf\u8ff0 Heap ID3D12Device::CreateDescriptorHeap \u521b\u5efa Heap\u3002 ID3D12GraphicsCommandList::SetDescriptorHeaps \u5c06 Heap \u7ed1\u5b9a\u5230 pipeline\u3002 typedef struct D3D12_DESCRIPTOR_HEAP_DESC { D3D12_DESCRIPTOR_HEAP_TYPE Type ; UINT NumDescriptors ; D3D12_DESCRIPTOR_HEAP_FLAGS Flags ; UINT NodeMask ; } D3D12_DESCRIPTOR_HEAP_DESC ; winrt :: com_ptr <:: ID3D12DescriptorHeap > d3D12DescriptorHeap ; D3D12_DESCRIPTOR_HEAP_DESC descriptorHeapDescription {}; descriptorHeapDescription . Type = D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV ; descriptorHeapDescription . NumDescriptors = descriptorCount ; descriptorHeapDescription . Flags = D3D12_DESCRIPTOR_HEAP_FLAG_SHADER_VISIBLE ; winrt :: check_hresult ( d3D12Device -> CreateDescriptorHeap ( & descriptorHeapDescription , _uuidof ( d3D12DescriptorHeap ), d3D12DescriptorHeap . put_void () ) ); std :: array < ID3D12DescriptorHeap * , 1 > d3D12DescriptorHeaps { d3D12DescriptorHeap . get () }; d3D12GraphicsCommandList -> SetDescriptorHeaps ( static_cast < UINT > ( d3D12DescriptorHeaps . size ()), d3D12DescriptorHeaps . data () ); \u521b\u5efa Binding Table Binding Table \u7528\u4e8e\u8868\u793a dispatchable \u7684\u8d44\u6e90\u3002Binding table \u6307\u5411 heap \u4e0a\u7684\u4e00\u5757\u533a\u57df\uff08DirectML \u5c06\u4f1a\u5728 heap \u8fd9\u5757\u533a\u57df\u4e2d\u5199\u5165 descriptors Input Output Temporary Persistent DML_BINDING_TABLE_DESC \u7528\u4e8e\u63cf\u8ff0 binding table struct DML_BINDING_TABLE_DESC { IDMLDispatchable * Dispatchable ; D3D12_CPU_DESCRIPTOR_HANDLE CPUDescriptorHandle ; D3D12_GPU_DESCRIPTOR_HANDLE GPUDescriptorHandle ; UINT SizeInDescriptors ; }; IDMLDevice::CreateBindingTable \u65b9\u6cd5\u521b\u5efa DirectML binding table\u3002\u4e4b\u540e\u6211\u4eec\u5c06\u521b\u5efa\u8d44\u6e90\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8d44\u6e90\u6dfb\u52a0\u5230 binding table\u3002 DML_BINDING_TABLE_DESC dmlBindingTableDesc {}; dmlBindingTableDesc . Dispatchable = dmlOperatorInitializer . get (); dmlBindingTableDesc . CPUDescriptorHandle = d3D12DescriptorHeap -> GetCPUDescriptorHandleForHeapStart (); dmlBindingTableDesc . GPUDescriptorHandle = d3D12DescriptorHeap -> GetGPUDescriptorHandleForHeapStart (); dmlBindingTableDesc . SizeInDescriptors = descriptorCount ; winrt :: com_ptr <:: IDMLBindingTable > dmlBindingTable ; winrt :: check_hresult ( dmlDevice -> CreateBindingTable ( & dmlBindingTableDesc , __uuidof ( dmlBindingTable ), dmlBindingTable . put_void () ) ); DirectML \u5c06 Descriptor \u5199\u5165 Heap \u7684\u987a\u5e8f\u672a\u5b9a\u4e49\uff0c\u6ce8\u610f\u4e0d\u8981\u8986\u5199 binding table \u4e2d\u7684 descriptor \u4ee3\u7801\u4e2d\u6ce8\u610f\u5728\u6267\u884cbinding table \u524d\uff0c\u4fdd\u8bc1 CPU descriptors \u62f7\u8d1d\u5230 GPU \u4e2d Handle \u652f\u6301 D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV \u7c7b\u578b\u7684 descriptor heap heap \u5fc5\u987b\u662f shader-visible \u7684 \u91cd\u7f6e binding table \u79fb\u9664\u6240\u6709\u8d44\u6e90 \u4fee\u6539 binding table \u63cf\u8ff0 IDMLBindingTable::Reset dmlBindingTableDesc . Dispatchable = pIDMLCompiledOperator . get (); winrt :: check_hresult ( pIDMLBindingTable -> Reset ( & dmlBindingTableDesc ) ); \u7ed1\u5b9a Temporary/Persistent \u4f7f\u7528\u4ece dispatchable \u4e2d\u83b7\u53d6\u7684 binding \u53c2\u6570\u586b\u5145 DML_BINDING_PROPERTIES \u7ed3\u6784\u3002\u521b\u5efa Direct3D 12 buffer \u8d44\u6e90\uff0c\u6dfb\u52a0\u5230 binding table \u4e2d\u3002 D3D12_HEAP_PROPERTIES defaultHeapProperties { CD3DX12_HEAP_PROPERTIES ( D3D12_HEAP_TYPE_DEFAULT ) }; winrt :: com_ptr <:: ID3D12Resource > temporaryBuffer ; D3D12_RESOURCE_DESC temporaryBufferDesc { CD3DX12_RESOURCE_DESC :: Buffer ( temporaryResourceSize ) }; winrt :: check_hresult ( d3D12Device -> CreateCommittedResource ( & defaultHeapProperties , D3D12_HEAP_FLAG_NONE , & temporaryBufferDesc , D3D12_RESOURCE_STATE_COMMON , nullptr , __uuidof ( temporaryBuffer ), temporaryBuffer . put_void () ) ); DML_BUFFER_BINDING bufferBinding { temporaryBuffer . get (), 0 , temporaryResourceSize }; DML_BINDING_DESC bindingDesc { DML_BINDING_TYPE_BUFFER , & bufferBinding }; dmlBindingTable -> BindTemporaryResource ( & bindingDesc ); temporary \u8d44\u6e90\u662f operator \u6267\u884c\u8fc7\u7a0b\u4e2d\u5185\u90e8\u4f7f\u7528\u7684\u5b58\u50a8\u8d44\u6e90\uff08GPU Memory\uff09\uff0c\u7a0b\u5e8f\u4e0d\u9700\u8981\u5173\u5fc3 temporary \u8d44\u6e90\u7684\u5185\u5bb9\uff0c\u4e5f\u4e0d\u9700\u8981\u5728 IDMLCommandRecorder::RecordDispatch \u6267\u884c\u540e\u4fdd\u7559\u5b83\u3002\u7a0b\u5e8f\u53ef\u4ee5\u9009\u62e9\u91ca\u653e\u8d44\u6e90\uff0c\u6216\u8005\u5728\u4e0d\u540c\u7684 dispatch \u4e0a\u91cd\u7528\u8fd9\u5757\u8d44\u6e90\u3002 \u5bf9\u4e8e persistent \u8d44\u6e90\uff0c\u521b\u5efa\u6d41\u7a0b\u4e0e\u4e0a\u8ff0\u4e00\u81f4\u3002\u4f46\u9700\u4f7f\u7528 IDMLBindingTable::BindOutputs \u5c06\u8d44\u6e90\u6dfb\u52a0\u5230 operator initializer \u7684 binding table \u4e2d\uff0c\u521d\u59cb\u5316 persistent \u8d44\u6e90\u662f initializer \u7684\u804c\u8d23\uff0c\u7136\u540e\u4f7f\u7528 IDMLBindingTable::BindPersistentResource \u7ed1\u5b9a\u5230 compiled operator\u3002persistent \u8d44\u6e90\u7684\u751f\u547d\u5468\u671f\u548c operator \u4e00\u6837\u957f\u3002 persistent \u8d44\u6e90\u901a\u5e38\u7528\u4e8e\u5b58\u50a8\u67e5\u627e\u8868\u6216 operator initialization \u8ba1\u7b97\u51fa\u7684\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u88ab operator \u4f7f\u7528\u3002 \u7ed1\u5b9a Tensors compiled operator \u9700\u4e3a binding table \u7ed1\u5b9a input \u548c output \u8d44\u6e90\u3002 \u7528 DML_BUFFER_BINDING \u548c DML_BINDING_DESC \u7ed3\u6784\u6765\u63cf\u8ff0 input output \u8d44\u6e90\uff0c\u4f7f\u7528 IDMLBindingTable::BindInputs \u548c IDMLBindingTable::BindOutputs \u6dfb\u52a0\u5230 binding table \u91cc\u3002\u8c03\u7528 IDMLBindingTable::Bind* \u65b9\u6cd5\u65f6, DirectML \u4f1a\u5c06 descriptors \u5199\u5165 CPU descriptors \u4e2d\u3002 DML_BUFFER_BINDING inputBufferBinding { inputBuffer . get (), 0 , tensorBufferSize }; DML_BINDING_DESC inputBindingDesc { DML_BINDING_TYPE_BUFFER , & inputBufferBinding }; dmlBindingTable -> BindInputs ( 1 , & inputBindingDesc ); DML_BUFFER_BINDING outputBufferBinding { outputBuffer . get (), 0 , tensorBufferSize }; DML_BINDING_DESC outputBindingDesc { DML_BINDING_TYPE_BUFFER , & outputBufferBinding }; dmlBindingTable -> BindOutputs ( 1 , & outputBindingDesc ); \u4f7f\u7528 DML_TENSOR_FLAG_OWNED_BY_DML \u6807\u8bc6\uff0c\u8ba9 DirectML \u7ba1\u7406 tensors\u3002DirectML \u5c06\u6570\u636e\u62f7\u8d1d\u3001\u5b58\u50a8\u5230 persistent \u8d44\u6e90\uff0cDirectML \u5c06\u6570\u636e\u8f6c\u5316\u4e3a\u66f4\u9ad8\u6548\u6267\u884c\u7684\u683c\u5f0f\u3002\u5bf9\u4e8e operator \u751f\u547d\u5468\u671f\u4e0d\u53d8\u7684\u6570\u636e\uff08\u4f8b\u5982\u6743\u91cd\uff09\uff0c\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u62e5\u6709\u8be5\u6807\u8bc6\u7684\u7684 tensor \u53ea\u80fd\u5728 initializer \u9636\u6bb5\u4f7f\u7528\u3002 \u5bf9\u4e8e\u975e DML_TENSOR_FLAG_OWNED_BY_DML \u6807\u8bc6\u7684 tensor \u5728\u6267\u884c operator \u65f6\u7ed1\u5b9a\u3002 \u6267\u884c Dispatchable \u4f7f\u7528 IDMLCommandRecorder::RecordDispatch \u4f20\u9012 binding table \u53c2\u6570\u3002 \u7a0b\u5e8f\u9700\u4fdd\u8bc1 CPU Descriptors \u590d\u5236\u5230\u4e86 GPU \u4e2d\u3002 winrt :: com_ptr <:: ID3D12GraphicsCommandList > d3D12GraphicsCommandList ; // Code to create a Direct3D 12 command list goes here. winrt :: com_ptr <:: IDMLCommandRecorder > dmlCommandRecorder ; // Code to create a DirectML command recorder goes here. dmlCommandRecorder -> RecordDispatch ( d3D12GraphicsCommandList . get (), dmlOperatorInitializer . get (), dmlBindingTable . get () ); \u5173\u95ed command list\uff0c\u63d0\u4ea4\u6267\u884c\u3002 GPU \u6267\u884c RecordDispatch \u524d\uff0c\u7a0b\u5e8f\u9700\u8981\u5c06\u6240\u6709\u7ed1\u5b9a\u7684\u8d44\u6e90\u8f6c\u6362\u5230 D3D12_RESOURCE_STATE_UNORDERED_ACCESS \u72b6\u6001\uff0c\u6216\u9690\u5f0f\u8f6c\u6362\u5230 D3D12_RESOURCE_STATE_UNORDERED_ACCESS \u7684\u72b6\u6001\uff0c\u4f8b\u5982 D3D12_RESOURCE_STATE_COMMON\u3002 \u4e0a\u8ff0\u4f8b\u5916\u662f upload heaps \u7ed1\u5b9a\u5230\u521d\u59cb\u5316 operator DML_TENSOR_FLAG_OWNED_BY_DML\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cupload heaps \u5e94\u5904\u5728 D3D12_RESOURCE_STATE_GENERIC_READ \u72b6\u6001\u3002 \u5bf9\u4e8e RecordDispatch\uff0c\u9700\u8981\u4f7f\u7528 unordered access view (UAV) barriers \u6765\u4fdd\u8bc1 dispatches \u95f4\u6b63\u786e\u6570\u636e\u4f9d\u8d56\u3002\u793a\u4f8b\u4ee3\u7801\u4e2d\u6ca1\u6709 UAV barriers\uff0c\u5f53 dispatch \u95f4\u6709\u6570\u636e\u4f9d\u8d56\uff08\u540e\u9762 dispatch \u7684 input \u7528\u5230\u4e86\u524d\u9762 dispatch \u7684 output\uff09\uff0c\u5b9e\u9645\u7a0b\u5e8f\u5fc5\u987b\u786e\u4fdd\u6b63\u786e\u7684 UAV barriers\u3002 Descriptors\u3001Binding Table \u7684\u751f\u547d\u5468\u671f\u548c\u540c\u6b65 \u5f53 Descriptor \u88ab\u4f7f\u7528\u65f6\uff08\u4f8b\u5982\uff0c\u88ab\u524d\u4e00\u5e27\u4f7f\u7528\uff09\uff0cbinding table \u65e0\u6cd5\u8986\u5199\uff0c\u7a0b\u5e8f\u9700\u8981\u7b49\u5f85 dispatchable GPU \u4e0a\u7ed3\u675f\u6267\u884c\u3002 Binding table \u4e0d\u6301\u6709 descriptor heap \u7684\u5f3a\u5f15\u7528\uff0c\u56e0\u800c\u7a0b\u5e8f\u5e94\u7b49\u5f85\u4f7f\u7528\u4e86\u8fd9\u4e2a binding table \u7684 dispatchable GPU \u6267\u884c\u7ed3\u675f\u540e\u518d\u91ca\u653e descriptor heap\u3002 Binding table \u4e0d\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff0c\u4e0d\u8981\u5728\u672a\u540c\u6b65\u60c5\u51b5\u4e0b\u591a\u7ebf\u7a0b\u4f7f\u7528 binding table\u3002 \u5bf9\u4e8e ML\u3001CG \u6df7\u5408\u5de5\u4f5c\u6d41\uff0c\u7a0b\u5e8f\u5e94\u4fdd\u8bc1 binding table \u6307\u5411\u7684 descriptor heap \u6ca1\u6709\u6b63\u88ab GPU \u4f7f\u7528\u3002 Binding in DirectML - Windows applications | Microsoft Docs \u21a9 directml.h header | Microsoft Docs \u21a9","title":"\u8d44\u6e90\u7ed1\u5b9a"},{"location":"directx/binding/#directml-1","text":"\u5c06\u8d44\u6e90\u7ed1\u5b9a\u5230 pipeline \u4e0a\uff0cDirectML \u8d44\u6e90\u5305\u542b\u4e0b\u9762\u51e0\u79cd Input Output Temporary Persistent \u672c\u6587\u5305\u542b\u7ed1\u5b9a\u7684\u6982\u5ff5\u53ca\u5b9e\u73b0\u7ec6\u8282","title":"DirectML \u4e2d\u7684\u8d44\u6e90\u7ed1\u5b9a1"},{"location":"directx/binding/#_1","text":"\u6267\u884c dispatchable\uff08operator initializer/ compiled operator\uff09\uff0c\u9700\u4f9d\u7167\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u3002 IDMLDispatchable::GetBindingProperties \u83b7\u53d6\u5f53\u524d dispatchable \u6240\u9700\u7684 descriptors \u6570\u91cf\uff0ctemporary/persistent \u8d44\u6e90 \u521b\u5efa\u8db3\u591f\u5bb9\u7eb3 descriptors \u7684 Direct3D 12 descriptor heap\uff0c\u7ed1\u5b9a\u5230 pipeline IDMLDevice::CreateBindingTable \u521b\u5efa DirectML \u7684 binding table\uff0c\u8868\u793a pipeline \u4e0a\u7684\u8d44\u6e90\u3002\u4f7f\u7528 DML_BINDING_TABLE_DESC \u7ed3\u6784\u6765\u63cf\u8ff0 binding table\u3002 \u521b\u5efa Direct3D 12 buffer \u683c\u5f0f\u7684 temporal/persistent \u8d44\u6e90\uff0c \u7528 DML_BUFFER_BINDING \u548c DML_BINDING_DESC \u63cf\u8ff0\uff0c\u6dfb\u52a0\u5230 binding table \u5f53 dispatchable \u4e3a compiled operator \u65f6\uff0c\u521b\u5efa Direct3D 12 buffer \u8d44\u6e90\uff0c\u6dfb\u52a0\u5230 binding table \u5c06 binding table \u4f5c\u4e3a\u53c2\u6570\u4f20\u5165 IDMLCommandRecorder::RecordDispatch","title":"\u91cd\u8981\u6982\u5ff5"},{"location":"directx/binding/#binding-properties","text":"Note \u76f8\u540c\u7684 operator \u53ef\u80fd\u4f1a\u6709\u4e0d\u540c\u7684 binding properties DML_BINDING_PROPERTIES \u7ed3\u6784\u5b9a\u4e49\u4e86 dispatchable \u7ed1\u5b9a\u8d44\u6e90\u76f8\u5173\u6570\u636e\uff0c\u4e0b\u9762\u662f\u7ed3\u6784\u7684\u5b8c\u6574\u5b9a\u4e49 struct DML_BINDING_PROPERTIES { UINT RequiredDescriptorCount ; UINT64 TemporaryResourceSize ; UINT64 PersistentResourceSize ; }; \u4f7f\u7528 IDMLDispatchable::GetBindingProperties \u83b7\u53d6\u6570\u636e winrt :: com_ptr <:: IDMLCompiledOperator > dmlCompiledOperator ; // Code to create and compile a DirectML operator goes here. DML_BINDING_PROPERTIES executeDmlBindingProperties { dmlCompiledOperator -> GetBindingProperties () }; winrt :: com_ptr <:: IDMLOperatorInitializer > dmlOperatorInitializer ; // Code to create a DirectML operator initializer goes here. DML_BINDING_PROPERTIES initializeDmlBindingProperties { dmlOperatorInitializer -> GetBindingProperties () }; UINT descriptorCount = ... RequiredDescriptorCount\uff1aDescriptor Heap size TemporaryResourceSize PersistentResourceSize","title":"\u83b7\u53d6 Binding Properties"},{"location":"directx/binding/#descriptor-heap","text":"DirectML \u5728 Heap \u4e0a\u521b\u5efa\u3001\u7ba1\u7406 Descriptors\u3002 D3D12_DESCRIPTOR_HEAP_DESC \u7ed3\u6784\u63cf\u8ff0 Heap ID3D12Device::CreateDescriptorHeap \u521b\u5efa Heap\u3002 ID3D12GraphicsCommandList::SetDescriptorHeaps \u5c06 Heap \u7ed1\u5b9a\u5230 pipeline\u3002 typedef struct D3D12_DESCRIPTOR_HEAP_DESC { D3D12_DESCRIPTOR_HEAP_TYPE Type ; UINT NumDescriptors ; D3D12_DESCRIPTOR_HEAP_FLAGS Flags ; UINT NodeMask ; } D3D12_DESCRIPTOR_HEAP_DESC ; winrt :: com_ptr <:: ID3D12DescriptorHeap > d3D12DescriptorHeap ; D3D12_DESCRIPTOR_HEAP_DESC descriptorHeapDescription {}; descriptorHeapDescription . Type = D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV ; descriptorHeapDescription . NumDescriptors = descriptorCount ; descriptorHeapDescription . Flags = D3D12_DESCRIPTOR_HEAP_FLAG_SHADER_VISIBLE ; winrt :: check_hresult ( d3D12Device -> CreateDescriptorHeap ( & descriptorHeapDescription , _uuidof ( d3D12DescriptorHeap ), d3D12DescriptorHeap . put_void () ) ); std :: array < ID3D12DescriptorHeap * , 1 > d3D12DescriptorHeaps { d3D12DescriptorHeap . get () }; d3D12GraphicsCommandList -> SetDescriptorHeaps ( static_cast < UINT > ( d3D12DescriptorHeaps . size ()), d3D12DescriptorHeaps . data () );","title":"\u521b\u5efa Descriptor Heap"},{"location":"directx/binding/#binding-table","text":"Binding Table \u7528\u4e8e\u8868\u793a dispatchable \u7684\u8d44\u6e90\u3002Binding table \u6307\u5411 heap \u4e0a\u7684\u4e00\u5757\u533a\u57df\uff08DirectML \u5c06\u4f1a\u5728 heap \u8fd9\u5757\u533a\u57df\u4e2d\u5199\u5165 descriptors Input Output Temporary Persistent DML_BINDING_TABLE_DESC \u7528\u4e8e\u63cf\u8ff0 binding table struct DML_BINDING_TABLE_DESC { IDMLDispatchable * Dispatchable ; D3D12_CPU_DESCRIPTOR_HANDLE CPUDescriptorHandle ; D3D12_GPU_DESCRIPTOR_HANDLE GPUDescriptorHandle ; UINT SizeInDescriptors ; }; IDMLDevice::CreateBindingTable \u65b9\u6cd5\u521b\u5efa DirectML binding table\u3002\u4e4b\u540e\u6211\u4eec\u5c06\u521b\u5efa\u8d44\u6e90\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8d44\u6e90\u6dfb\u52a0\u5230 binding table\u3002 DML_BINDING_TABLE_DESC dmlBindingTableDesc {}; dmlBindingTableDesc . Dispatchable = dmlOperatorInitializer . get (); dmlBindingTableDesc . CPUDescriptorHandle = d3D12DescriptorHeap -> GetCPUDescriptorHandleForHeapStart (); dmlBindingTableDesc . GPUDescriptorHandle = d3D12DescriptorHeap -> GetGPUDescriptorHandleForHeapStart (); dmlBindingTableDesc . SizeInDescriptors = descriptorCount ; winrt :: com_ptr <:: IDMLBindingTable > dmlBindingTable ; winrt :: check_hresult ( dmlDevice -> CreateBindingTable ( & dmlBindingTableDesc , __uuidof ( dmlBindingTable ), dmlBindingTable . put_void () ) ); DirectML \u5c06 Descriptor \u5199\u5165 Heap \u7684\u987a\u5e8f\u672a\u5b9a\u4e49\uff0c\u6ce8\u610f\u4e0d\u8981\u8986\u5199 binding table \u4e2d\u7684 descriptor \u4ee3\u7801\u4e2d\u6ce8\u610f\u5728\u6267\u884cbinding table \u524d\uff0c\u4fdd\u8bc1 CPU descriptors \u62f7\u8d1d\u5230 GPU \u4e2d Handle \u652f\u6301 D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV \u7c7b\u578b\u7684 descriptor heap heap \u5fc5\u987b\u662f shader-visible \u7684 \u91cd\u7f6e binding table \u79fb\u9664\u6240\u6709\u8d44\u6e90 \u4fee\u6539 binding table \u63cf\u8ff0 IDMLBindingTable::Reset dmlBindingTableDesc . Dispatchable = pIDMLCompiledOperator . get (); winrt :: check_hresult ( pIDMLBindingTable -> Reset ( & dmlBindingTableDesc ) );","title":"\u521b\u5efa Binding Table"},{"location":"directx/binding/#temporarypersistent","text":"\u4f7f\u7528\u4ece dispatchable \u4e2d\u83b7\u53d6\u7684 binding \u53c2\u6570\u586b\u5145 DML_BINDING_PROPERTIES \u7ed3\u6784\u3002\u521b\u5efa Direct3D 12 buffer \u8d44\u6e90\uff0c\u6dfb\u52a0\u5230 binding table \u4e2d\u3002 D3D12_HEAP_PROPERTIES defaultHeapProperties { CD3DX12_HEAP_PROPERTIES ( D3D12_HEAP_TYPE_DEFAULT ) }; winrt :: com_ptr <:: ID3D12Resource > temporaryBuffer ; D3D12_RESOURCE_DESC temporaryBufferDesc { CD3DX12_RESOURCE_DESC :: Buffer ( temporaryResourceSize ) }; winrt :: check_hresult ( d3D12Device -> CreateCommittedResource ( & defaultHeapProperties , D3D12_HEAP_FLAG_NONE , & temporaryBufferDesc , D3D12_RESOURCE_STATE_COMMON , nullptr , __uuidof ( temporaryBuffer ), temporaryBuffer . put_void () ) ); DML_BUFFER_BINDING bufferBinding { temporaryBuffer . get (), 0 , temporaryResourceSize }; DML_BINDING_DESC bindingDesc { DML_BINDING_TYPE_BUFFER , & bufferBinding }; dmlBindingTable -> BindTemporaryResource ( & bindingDesc ); temporary \u8d44\u6e90\u662f operator \u6267\u884c\u8fc7\u7a0b\u4e2d\u5185\u90e8\u4f7f\u7528\u7684\u5b58\u50a8\u8d44\u6e90\uff08GPU Memory\uff09\uff0c\u7a0b\u5e8f\u4e0d\u9700\u8981\u5173\u5fc3 temporary \u8d44\u6e90\u7684\u5185\u5bb9\uff0c\u4e5f\u4e0d\u9700\u8981\u5728 IDMLCommandRecorder::RecordDispatch \u6267\u884c\u540e\u4fdd\u7559\u5b83\u3002\u7a0b\u5e8f\u53ef\u4ee5\u9009\u62e9\u91ca\u653e\u8d44\u6e90\uff0c\u6216\u8005\u5728\u4e0d\u540c\u7684 dispatch \u4e0a\u91cd\u7528\u8fd9\u5757\u8d44\u6e90\u3002 \u5bf9\u4e8e persistent \u8d44\u6e90\uff0c\u521b\u5efa\u6d41\u7a0b\u4e0e\u4e0a\u8ff0\u4e00\u81f4\u3002\u4f46\u9700\u4f7f\u7528 IDMLBindingTable::BindOutputs \u5c06\u8d44\u6e90\u6dfb\u52a0\u5230 operator initializer \u7684 binding table \u4e2d\uff0c\u521d\u59cb\u5316 persistent \u8d44\u6e90\u662f initializer \u7684\u804c\u8d23\uff0c\u7136\u540e\u4f7f\u7528 IDMLBindingTable::BindPersistentResource \u7ed1\u5b9a\u5230 compiled operator\u3002persistent \u8d44\u6e90\u7684\u751f\u547d\u5468\u671f\u548c operator \u4e00\u6837\u957f\u3002 persistent \u8d44\u6e90\u901a\u5e38\u7528\u4e8e\u5b58\u50a8\u67e5\u627e\u8868\u6216 operator initialization \u8ba1\u7b97\u51fa\u7684\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u88ab operator \u4f7f\u7528\u3002","title":"\u7ed1\u5b9a Temporary/Persistent"},{"location":"directx/binding/#tensors","text":"compiled operator \u9700\u4e3a binding table \u7ed1\u5b9a input \u548c output \u8d44\u6e90\u3002 \u7528 DML_BUFFER_BINDING \u548c DML_BINDING_DESC \u7ed3\u6784\u6765\u63cf\u8ff0 input output \u8d44\u6e90\uff0c\u4f7f\u7528 IDMLBindingTable::BindInputs \u548c IDMLBindingTable::BindOutputs \u6dfb\u52a0\u5230 binding table \u91cc\u3002\u8c03\u7528 IDMLBindingTable::Bind* \u65b9\u6cd5\u65f6, DirectML \u4f1a\u5c06 descriptors \u5199\u5165 CPU descriptors \u4e2d\u3002 DML_BUFFER_BINDING inputBufferBinding { inputBuffer . get (), 0 , tensorBufferSize }; DML_BINDING_DESC inputBindingDesc { DML_BINDING_TYPE_BUFFER , & inputBufferBinding }; dmlBindingTable -> BindInputs ( 1 , & inputBindingDesc ); DML_BUFFER_BINDING outputBufferBinding { outputBuffer . get (), 0 , tensorBufferSize }; DML_BINDING_DESC outputBindingDesc { DML_BINDING_TYPE_BUFFER , & outputBufferBinding }; dmlBindingTable -> BindOutputs ( 1 , & outputBindingDesc ); \u4f7f\u7528 DML_TENSOR_FLAG_OWNED_BY_DML \u6807\u8bc6\uff0c\u8ba9 DirectML \u7ba1\u7406 tensors\u3002DirectML \u5c06\u6570\u636e\u62f7\u8d1d\u3001\u5b58\u50a8\u5230 persistent \u8d44\u6e90\uff0cDirectML \u5c06\u6570\u636e\u8f6c\u5316\u4e3a\u66f4\u9ad8\u6548\u6267\u884c\u7684\u683c\u5f0f\u3002\u5bf9\u4e8e operator \u751f\u547d\u5468\u671f\u4e0d\u53d8\u7684\u6570\u636e\uff08\u4f8b\u5982\u6743\u91cd\uff09\uff0c\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u62e5\u6709\u8be5\u6807\u8bc6\u7684\u7684 tensor \u53ea\u80fd\u5728 initializer \u9636\u6bb5\u4f7f\u7528\u3002 \u5bf9\u4e8e\u975e DML_TENSOR_FLAG_OWNED_BY_DML \u6807\u8bc6\u7684 tensor \u5728\u6267\u884c operator \u65f6\u7ed1\u5b9a\u3002","title":"\u7ed1\u5b9a Tensors"},{"location":"directx/binding/#dispatchable","text":"\u4f7f\u7528 IDMLCommandRecorder::RecordDispatch \u4f20\u9012 binding table \u53c2\u6570\u3002 \u7a0b\u5e8f\u9700\u4fdd\u8bc1 CPU Descriptors \u590d\u5236\u5230\u4e86 GPU \u4e2d\u3002 winrt :: com_ptr <:: ID3D12GraphicsCommandList > d3D12GraphicsCommandList ; // Code to create a Direct3D 12 command list goes here. winrt :: com_ptr <:: IDMLCommandRecorder > dmlCommandRecorder ; // Code to create a DirectML command recorder goes here. dmlCommandRecorder -> RecordDispatch ( d3D12GraphicsCommandList . get (), dmlOperatorInitializer . get (), dmlBindingTable . get () ); \u5173\u95ed command list\uff0c\u63d0\u4ea4\u6267\u884c\u3002 GPU \u6267\u884c RecordDispatch \u524d\uff0c\u7a0b\u5e8f\u9700\u8981\u5c06\u6240\u6709\u7ed1\u5b9a\u7684\u8d44\u6e90\u8f6c\u6362\u5230 D3D12_RESOURCE_STATE_UNORDERED_ACCESS \u72b6\u6001\uff0c\u6216\u9690\u5f0f\u8f6c\u6362\u5230 D3D12_RESOURCE_STATE_UNORDERED_ACCESS \u7684\u72b6\u6001\uff0c\u4f8b\u5982 D3D12_RESOURCE_STATE_COMMON\u3002 \u4e0a\u8ff0\u4f8b\u5916\u662f upload heaps \u7ed1\u5b9a\u5230\u521d\u59cb\u5316 operator DML_TENSOR_FLAG_OWNED_BY_DML\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cupload heaps \u5e94\u5904\u5728 D3D12_RESOURCE_STATE_GENERIC_READ \u72b6\u6001\u3002 \u5bf9\u4e8e RecordDispatch\uff0c\u9700\u8981\u4f7f\u7528 unordered access view (UAV) barriers \u6765\u4fdd\u8bc1 dispatches \u95f4\u6b63\u786e\u6570\u636e\u4f9d\u8d56\u3002\u793a\u4f8b\u4ee3\u7801\u4e2d\u6ca1\u6709 UAV barriers\uff0c\u5f53 dispatch \u95f4\u6709\u6570\u636e\u4f9d\u8d56\uff08\u540e\u9762 dispatch \u7684 input \u7528\u5230\u4e86\u524d\u9762 dispatch \u7684 output\uff09\uff0c\u5b9e\u9645\u7a0b\u5e8f\u5fc5\u987b\u786e\u4fdd\u6b63\u786e\u7684 UAV barriers\u3002","title":"\u6267\u884c Dispatchable"},{"location":"directx/binding/#descriptorsbinding-table","text":"\u5f53 Descriptor \u88ab\u4f7f\u7528\u65f6\uff08\u4f8b\u5982\uff0c\u88ab\u524d\u4e00\u5e27\u4f7f\u7528\uff09\uff0cbinding table \u65e0\u6cd5\u8986\u5199\uff0c\u7a0b\u5e8f\u9700\u8981\u7b49\u5f85 dispatchable GPU \u4e0a\u7ed3\u675f\u6267\u884c\u3002 Binding table \u4e0d\u6301\u6709 descriptor heap \u7684\u5f3a\u5f15\u7528\uff0c\u56e0\u800c\u7a0b\u5e8f\u5e94\u7b49\u5f85\u4f7f\u7528\u4e86\u8fd9\u4e2a binding table \u7684 dispatchable GPU \u6267\u884c\u7ed3\u675f\u540e\u518d\u91ca\u653e descriptor heap\u3002 Binding table \u4e0d\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff0c\u4e0d\u8981\u5728\u672a\u540c\u6b65\u60c5\u51b5\u4e0b\u591a\u7ebf\u7a0b\u4f7f\u7528 binding table\u3002 \u5bf9\u4e8e ML\u3001CG \u6df7\u5408\u5de5\u4f5c\u6d41\uff0c\u7a0b\u5e8f\u5e94\u4fdd\u8bc1 binding table \u6307\u5411\u7684 descriptor heap \u6ca1\u6709\u6b63\u88ab GPU \u4f7f\u7528\u3002 Binding in DirectML - Windows applications | Microsoft Docs \u21a9 directml.h header | Microsoft Docs \u21a9","title":"Descriptors\u3001Binding Table \u7684\u751f\u547d\u5468\u671f\u548c\u540c\u6b65"},{"location":"directx/directml/","text":"DirectML \u5b98\u65b9\u6982\u8ff0 Direct Machine Learning (DirectML) is a low-level API for machine learning (ML). Hardware-accelerated machine learning primitives (called operators) are the building blocks of DirectML. From those building blocks, you can develop such machine learning techniques as upscaling, anti-aliasing, and style transfer, to name but a few. Denoising and super-resolution, for example, allow you to achieve impressive raytraced effects with fewer rays per pixel. Prerequisites DirectX 12-capable GPU drivers Windows 10 version 1903 or newer Windows 10 SDK version 1903 DirectML Resource The key to resource binding in DirectX 12 are the concepts of a descriptor, descriptor tables, descriptor heaps, and a root signature. A descriptor is a small object that contains information about one resource. typedef struct D3D12_SHADER_RESOURCE_VIEW_DESC { DXGI_FORMAT Format ; D3D12_SRV_DIMENSION ViewDimension ; union { D3D12_BUFFER_SRV Buffer ; D3D12_TEX1D_SRV Texture1D ; D3D12_TEX1D_ARRAY_SRV Texture1DArray ; D3D12_TEX2D_SRV Texture2D ; D3D12_TEX2D_ARRAY_SRV Texture2DArray ; D3D12_TEX2DMS_SRV Texture2DMS ; D3D12_TEX2DMS_ARRAY_SRV Texture2DMSArray ; D3D12_TEX3D_SRV Texture3D ; D3D12_TEXCUBE_SRV TextureCube ; D3D12_TEXCUBE_ARRAY_SRV TextureCubeArray ; D3D12_BUFFEREX_SRV BufferEx ; }; } D3D12_SHADER_RESOURCE_VIEW_DESC ; interface ID3D12Device { ... void CreateShaderResourceView ( _In_opt_ ID3D12Resource * pResource , _In_opt_ const D3D12_SHADER_RESOURCE_VIEW_DESC * pDesc , _In_ D3D12_CPU_DESCRIPTOR_HANDLE DestDescriptor ); }; // create SRV D3D12_SHADER_RESOURCE_VIEW_DESC srvDesc ; ZeroMemory ( & srvDesc , sizeof ( D3D12_SHADER_RESOURCE_VIEW_DESC )); srvDesc . Format = mTexture -> Format ; srvDesc . ViewDimension = D3D12_SRV_DIMENSION_TEXTURE2D ; srvDesc . Texture2D . MipLevels = 1 ; mDevice -> CreateShaderResourceView ( mTexture . Get (), & srvDesc , mCbvSrvDescriptorHeap -> GetCPUDescriptorHandleForHeapStart ()); Descriptor\uff1a \u63cf\u8ff0 GPU \u4e2d\u8d44\u6e90\u7c7b\u578b\u3001\u6570\u636e\u683c\u5f0f\u3001\u5b58\u50a8\u5730\u5740\u3002 \u5e38\u89c1\u7684 Descriptor \u7c7b\u578b: Constant buffer views (CBVs) Unordered access views (UAVs) Shader resource views (SRVs) Samplers Descriptor Table\uff1aDescriptors \u5206\u7c7b\u5b58\u5165 Descriptor Table Descriptor Heap\uff1aDescriptors \u5b58\u50a8\u5728 Descriptor Heap \u4e2d Topic Description Descriptor Descriptor Heap Descriptor Table Root Singature Descriptor \u521b\u5efa\u6d41\u7a0b\uff1a GPU \u521b\u5efa Descriptor Heap GPU \u521b\u5efa\u8d44\u6e90 GPU \u5229\u7528 1. \u521b\u5efa Descriptor Table \uff08\u53ef\u9009\uff09 GPU \u5229\u7528 1.2. \u7684\u4fe1\u606f\uff0c\u5728 Descriptor Heap / Table \u4e0a\u521b\u5efa Descriptor Windows provides APIs and components that support graphics, gaming, and imaging. \u21a9 Introduction to Resource Binding in Microsoft DirectX 12 \u21a9 Microsoft DirectX* 12 \u4e2d\u8d44\u6e90\u7ed1\u5b9a\u7684\u6027\u80fd\u8003\u8651\u56e0\u7d20 \u21a9 DirectML API \u6587\u6863 \u21a9","title":"DirectML \u6307\u5357"},{"location":"directx/directml/#prerequisites","text":"DirectX 12-capable GPU drivers Windows 10 version 1903 or newer Windows 10 SDK version 1903","title":"Prerequisites"},{"location":"directx/directml/#directml-resource","text":"The key to resource binding in DirectX 12 are the concepts of a descriptor, descriptor tables, descriptor heaps, and a root signature. A descriptor is a small object that contains information about one resource. typedef struct D3D12_SHADER_RESOURCE_VIEW_DESC { DXGI_FORMAT Format ; D3D12_SRV_DIMENSION ViewDimension ; union { D3D12_BUFFER_SRV Buffer ; D3D12_TEX1D_SRV Texture1D ; D3D12_TEX1D_ARRAY_SRV Texture1DArray ; D3D12_TEX2D_SRV Texture2D ; D3D12_TEX2D_ARRAY_SRV Texture2DArray ; D3D12_TEX2DMS_SRV Texture2DMS ; D3D12_TEX2DMS_ARRAY_SRV Texture2DMSArray ; D3D12_TEX3D_SRV Texture3D ; D3D12_TEXCUBE_SRV TextureCube ; D3D12_TEXCUBE_ARRAY_SRV TextureCubeArray ; D3D12_BUFFEREX_SRV BufferEx ; }; } D3D12_SHADER_RESOURCE_VIEW_DESC ; interface ID3D12Device { ... void CreateShaderResourceView ( _In_opt_ ID3D12Resource * pResource , _In_opt_ const D3D12_SHADER_RESOURCE_VIEW_DESC * pDesc , _In_ D3D12_CPU_DESCRIPTOR_HANDLE DestDescriptor ); }; // create SRV D3D12_SHADER_RESOURCE_VIEW_DESC srvDesc ; ZeroMemory ( & srvDesc , sizeof ( D3D12_SHADER_RESOURCE_VIEW_DESC )); srvDesc . Format = mTexture -> Format ; srvDesc . ViewDimension = D3D12_SRV_DIMENSION_TEXTURE2D ; srvDesc . Texture2D . MipLevels = 1 ; mDevice -> CreateShaderResourceView ( mTexture . Get (), & srvDesc , mCbvSrvDescriptorHeap -> GetCPUDescriptorHandleForHeapStart ()); Descriptor\uff1a \u63cf\u8ff0 GPU \u4e2d\u8d44\u6e90\u7c7b\u578b\u3001\u6570\u636e\u683c\u5f0f\u3001\u5b58\u50a8\u5730\u5740\u3002 \u5e38\u89c1\u7684 Descriptor \u7c7b\u578b: Constant buffer views (CBVs) Unordered access views (UAVs) Shader resource views (SRVs) Samplers Descriptor Table\uff1aDescriptors \u5206\u7c7b\u5b58\u5165 Descriptor Table Descriptor Heap\uff1aDescriptors \u5b58\u50a8\u5728 Descriptor Heap \u4e2d Topic Description Descriptor Descriptor Heap Descriptor Table Root Singature Descriptor \u521b\u5efa\u6d41\u7a0b\uff1a GPU \u521b\u5efa Descriptor Heap GPU \u521b\u5efa\u8d44\u6e90 GPU \u5229\u7528 1. \u521b\u5efa Descriptor Table \uff08\u53ef\u9009\uff09 GPU \u5229\u7528 1.2. \u7684\u4fe1\u606f\uff0c\u5728 Descriptor Heap / Table \u4e0a\u521b\u5efa Descriptor Windows provides APIs and components that support graphics, gaming, and imaging. \u21a9 Introduction to Resource Binding in Microsoft DirectX 12 \u21a9 Microsoft DirectX* 12 \u4e2d\u8d44\u6e90\u7ed1\u5b9a\u7684\u6027\u80fd\u8003\u8651\u56e0\u7d20 \u21a9 DirectML API \u6587\u6863 \u21a9","title":"DirectML Resource"},{"location":"directx/project/","text":"\u9879\u76ee\u7f16\u7801\u76f8\u5173 \u5c1d\u8bd5\u7528 Join Operator 1 \u8fdb\u884c\u7c7b\u4f3c torch.cat \u7684\u64cd\u4f5c struct DML_JOIN_OPERATOR_DESC { UINT InputCount ; _In_reads_ ( InputCount ) const DML_TENSOR_DESC * InputTensors ; const DML_TENSOR_DESC * OutputTensor ; UINT Axis ; }; Persistent Temporary \u8d44\u6e90\u8bf4\u660e Persistent \u8d44\u6e90\u4e0e compiled operator \u751f\u547d\u5468\u671f\u4e00\u81f4\u3002\u5982\u679c operator \u8bf7\u6c42 persistent \u8d44\u6e90\uff0c\u90a3\u4e48\u7a0b\u5e8f\u5fc5\u987b\u5728 initializer operator \u4e0a\u63d0\u4f9b\uff0c\u5e76\u4e14\u5728\u4e4b\u540e\u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\u8fd9\u5757\u8d44\u6e90\u4e0d\u80fd\u88ab\u4fee\u6539\u3002 2 \u5bf9\u4e8e initializer operator\uff0coutput \u4e0a\u7684\u7ed1\u5b9a\u4ee3\u8868\u4e86 operator \u7684 persistent \u8d44\u6e90\u3002 3 DML_TENSOR_FLAG_OWNED_BY_DML \u6807\u8bc6\u7528\u4e8e\u6743\u91cd\u3002 If the dispatchable reports a non-zero size for its more long-lived persistent resource, though, then the procedure is a little different. You should create a buffer and describe a binding following the same pattern as shown above. But add it to your operator initializer's binding table with a call to IDMLBindingTable::BindOutputs, because it's the operator initializer's job to initialize the persistent resource. Then add it to your compiled operator's binding table with a call to IDMLBindingTable::BindPersistentResource. 4 For operator initializers, input bindings are expected to be of type DML_BINDING_TYPE_BUFFER_ARRAY with one input binding per operator to initialize, supplied in the order that you specified the operators during creation or reset of the initializer. Each buffer array should have a size equal to the number of inputs of its corresponding operator to initialize. Input tensors that had the DML_TENSOR_FLAG_OWNED_BY_DML flag set should be bound during initialize\u2014otherwise, nothing should be bound for that tensor. If there is nothing to be bound as input for initialization of an operator (that is, there are no tensors with the DML_TENSOR_FLAG_OWNED_BY_DML flag set) then you may supply nullptr or an empty DML_BUFFER_ARRAY_BINDING to indicate 'no binding'. 5 Persistent \u8d44\u6e90\u4f7f\u7528\u65b9\u6cd5\uff1a \u521b\u5efa persistent \u8d44\u6e90 \u521d\u59cb\u5316\u65f6\u5c06 persistent \u8d44\u6e90\u7ed1\u5b9a\u5230 initializer operator \u7684 outputs\uff08\u6267\u884c\u521d\u59cb\u5316\u524d\u9700\u8981\u5c06 weight tensors \u7ed1\u5b9a\u5230 inputs\uff09 \u6267\u884c\u65f6\u7ed1\u5b9a\u5230 compiled operator \u7684 persistent \u5377\u79ef Operator struct DML_CONVOLUTION_OPERATOR_DESC { const DML_TENSOR_DESC * InputTensor ; const DML_TENSOR_DESC * FilterTensor ; const DML_TENSOR_DESC * BiasTensor ; const DML_TENSOR_DESC * OutputTensor ; DML_CONVOLUTION_MODE Mode ; DML_CONVOLUTION_DIRECTION Direction ; UINT DimensionCount ; const UINT * Strides ; const UINT * Dilations ; const UINT * StartPadding ; const UINT * EndPadding ; const UINT * OutputPadding ; UINT GroupCount ; const DML_OPERATOR_DESC * FusedActivation ; }; \u53c2\u6570 \u8bf4\u660e InputTensor - FilterTensor - BiasTensor \u53ef\u9009\u53c2\u6570 OutputTensor - Mode \u4f7f\u7528 DML_CONVOLUTION_MODE_CROSS_CORRELATION\uff0c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5377\u79ef\u5373\u4e92\u76f8\u5173 Direction forward pass/backward pass DimensionCount The number of dimensions. This field determines the size of the Strides, Dilations, StartPadding, EndPadding, and OutputPadding arrays. Strides \u6b65\u957f Dilations \u6bcf\u4e00\u4e2a\u8f74\u7684\u81a8\u80c0\u7387\uff0c\u8bbe\u7f6e\u4e3a1\u8868\u793a\u4e0d\u81a8\u80c0 StartPadding \u8d77\u59cb\u4f4d\u7f6e\u9700\u8981\u6dfb\u52a0\u7684 pixels \u6570\u91cf EndPadding \u7ed3\u675f\u4f4d\u7f6e\u9700\u6dfb\u52a0\u7684 pixels \u6570\u91cf OutputPadding \u53cd\u5377\u79ef\u4e2d\u7528\u5230\u7684\u53c2\u6570 Group FusedActivation \u53ef\u9009\u53c2\u6570\uff0c\u6fc0\u6d3b\u5c42 Conv Operator \u5de5\u4f5c\u6d41 \u521d\u59cb\u5316 \u521b\u5efa D3D12 device\u3001queue\u3001list\u3001heaps\uff0cDML device \u521b\u5efa Operator \u521b\u5efa InputTensor\uff0cFilterTensor\uff0cBiasTensor\uff08\u53ef\u9009\uff09\uff0cOutputTensor Tensor \u63cf\u8ff0\u7b26 \u7528 1. \u4e2d\u63cf\u8ff0\u7b26\uff0c\u521b\u5efa DML_CONVOLUTION_OPERATOR_DESC \u521b\u5efa Operator \u521d\u59cb\u5316 Operator \u521b\u5efa Temporary\u3001Persistent \u8d44\u6e90 \u7ed1\u5b9a Temporary \u8d44\u6e90\uff08Persistent \u4ec5\u9700\u8981\u6267\u884c Operator \u65f6\u7ed1\u5b9a\uff09 \u4e3a binding table \u7ed1\u5b9a Outputs\uff08\u6b64\u5904\u4e3a Persistent \u8d44\u6e90 \u4e3a binding table \u7ed1\u5b9a Inputs \u521d\u59cb\u5316 Operator \u6267\u884c Operator \u7ed1\u5b9a Temporary\u3001Persistent \u8d44\u6e90 \u4e3a binding table \u7ed1\u5b9a Inputs \u4e3a binding table \u7ed1\u5b9a InputsOutputs \u6267\u884c Operator \u7ed3\u675f \u56de\u8bfb\u7ed3\u679c DirectML API \u6587\u6863 \u21a9 https://docs.microsoft.com/en-us/windows/desktop/api/directml/nf-directml-idmlbindingtable-bindpersistentresource \u21a9 https://docs.microsoft.com/en-us/windows/desktop/api/directml/nf-directml-idmlbindingtable-bindoutputs \u21a9 https://docs.microsoft.com/en-us/windows/desktop/direct3d12/dml-binding#describe-and-bind-any-temporarypersistent-resources \u21a9 https://docs.microsoft.com/en-us/windows/desktop/api/directml/nf-directml-idmlbindingtable-bindinputs \u21a9","title":"\u9879\u76ee\u7f16\u7801\u76f8\u5173"},{"location":"directx/project/#_1","text":"\u5c1d\u8bd5\u7528 Join Operator 1 \u8fdb\u884c\u7c7b\u4f3c torch.cat \u7684\u64cd\u4f5c struct DML_JOIN_OPERATOR_DESC { UINT InputCount ; _In_reads_ ( InputCount ) const DML_TENSOR_DESC * InputTensors ; const DML_TENSOR_DESC * OutputTensor ; UINT Axis ; };","title":"\u9879\u76ee\u7f16\u7801\u76f8\u5173"},{"location":"directx/project/#persistent-temporary","text":"Persistent \u8d44\u6e90\u4e0e compiled operator \u751f\u547d\u5468\u671f\u4e00\u81f4\u3002\u5982\u679c operator \u8bf7\u6c42 persistent \u8d44\u6e90\uff0c\u90a3\u4e48\u7a0b\u5e8f\u5fc5\u987b\u5728 initializer operator \u4e0a\u63d0\u4f9b\uff0c\u5e76\u4e14\u5728\u4e4b\u540e\u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\u8fd9\u5757\u8d44\u6e90\u4e0d\u80fd\u88ab\u4fee\u6539\u3002 2 \u5bf9\u4e8e initializer operator\uff0coutput \u4e0a\u7684\u7ed1\u5b9a\u4ee3\u8868\u4e86 operator \u7684 persistent \u8d44\u6e90\u3002 3 DML_TENSOR_FLAG_OWNED_BY_DML \u6807\u8bc6\u7528\u4e8e\u6743\u91cd\u3002 If the dispatchable reports a non-zero size for its more long-lived persistent resource, though, then the procedure is a little different. You should create a buffer and describe a binding following the same pattern as shown above. But add it to your operator initializer's binding table with a call to IDMLBindingTable::BindOutputs, because it's the operator initializer's job to initialize the persistent resource. Then add it to your compiled operator's binding table with a call to IDMLBindingTable::BindPersistentResource. 4 For operator initializers, input bindings are expected to be of type DML_BINDING_TYPE_BUFFER_ARRAY with one input binding per operator to initialize, supplied in the order that you specified the operators during creation or reset of the initializer. Each buffer array should have a size equal to the number of inputs of its corresponding operator to initialize. Input tensors that had the DML_TENSOR_FLAG_OWNED_BY_DML flag set should be bound during initialize\u2014otherwise, nothing should be bound for that tensor. If there is nothing to be bound as input for initialization of an operator (that is, there are no tensors with the DML_TENSOR_FLAG_OWNED_BY_DML flag set) then you may supply nullptr or an empty DML_BUFFER_ARRAY_BINDING to indicate 'no binding'. 5 Persistent \u8d44\u6e90\u4f7f\u7528\u65b9\u6cd5\uff1a \u521b\u5efa persistent \u8d44\u6e90 \u521d\u59cb\u5316\u65f6\u5c06 persistent \u8d44\u6e90\u7ed1\u5b9a\u5230 initializer operator \u7684 outputs\uff08\u6267\u884c\u521d\u59cb\u5316\u524d\u9700\u8981\u5c06 weight tensors \u7ed1\u5b9a\u5230 inputs\uff09 \u6267\u884c\u65f6\u7ed1\u5b9a\u5230 compiled operator \u7684 persistent","title":"Persistent Temporary \u8d44\u6e90\u8bf4\u660e"},{"location":"directx/project/#operator","text":"struct DML_CONVOLUTION_OPERATOR_DESC { const DML_TENSOR_DESC * InputTensor ; const DML_TENSOR_DESC * FilterTensor ; const DML_TENSOR_DESC * BiasTensor ; const DML_TENSOR_DESC * OutputTensor ; DML_CONVOLUTION_MODE Mode ; DML_CONVOLUTION_DIRECTION Direction ; UINT DimensionCount ; const UINT * Strides ; const UINT * Dilations ; const UINT * StartPadding ; const UINT * EndPadding ; const UINT * OutputPadding ; UINT GroupCount ; const DML_OPERATOR_DESC * FusedActivation ; }; \u53c2\u6570 \u8bf4\u660e InputTensor - FilterTensor - BiasTensor \u53ef\u9009\u53c2\u6570 OutputTensor - Mode \u4f7f\u7528 DML_CONVOLUTION_MODE_CROSS_CORRELATION\uff0c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5377\u79ef\u5373\u4e92\u76f8\u5173 Direction forward pass/backward pass DimensionCount The number of dimensions. This field determines the size of the Strides, Dilations, StartPadding, EndPadding, and OutputPadding arrays. Strides \u6b65\u957f Dilations \u6bcf\u4e00\u4e2a\u8f74\u7684\u81a8\u80c0\u7387\uff0c\u8bbe\u7f6e\u4e3a1\u8868\u793a\u4e0d\u81a8\u80c0 StartPadding \u8d77\u59cb\u4f4d\u7f6e\u9700\u8981\u6dfb\u52a0\u7684 pixels \u6570\u91cf EndPadding \u7ed3\u675f\u4f4d\u7f6e\u9700\u6dfb\u52a0\u7684 pixels \u6570\u91cf OutputPadding \u53cd\u5377\u79ef\u4e2d\u7528\u5230\u7684\u53c2\u6570 Group FusedActivation \u53ef\u9009\u53c2\u6570\uff0c\u6fc0\u6d3b\u5c42","title":"\u5377\u79ef Operator"},{"location":"directx/project/#conv-operator","text":"","title":"Conv Operator \u5de5\u4f5c\u6d41"},{"location":"directx/project/#_2","text":"\u521b\u5efa D3D12 device\u3001queue\u3001list\u3001heaps\uff0cDML device","title":"\u521d\u59cb\u5316"},{"location":"directx/project/#operator_1","text":"\u521b\u5efa InputTensor\uff0cFilterTensor\uff0cBiasTensor\uff08\u53ef\u9009\uff09\uff0cOutputTensor Tensor \u63cf\u8ff0\u7b26 \u7528 1. \u4e2d\u63cf\u8ff0\u7b26\uff0c\u521b\u5efa DML_CONVOLUTION_OPERATOR_DESC \u521b\u5efa Operator","title":"\u521b\u5efa Operator"},{"location":"directx/project/#operator_2","text":"\u521b\u5efa Temporary\u3001Persistent \u8d44\u6e90 \u7ed1\u5b9a Temporary \u8d44\u6e90\uff08Persistent \u4ec5\u9700\u8981\u6267\u884c Operator \u65f6\u7ed1\u5b9a\uff09 \u4e3a binding table \u7ed1\u5b9a Outputs\uff08\u6b64\u5904\u4e3a Persistent \u8d44\u6e90 \u4e3a binding table \u7ed1\u5b9a Inputs \u521d\u59cb\u5316 Operator","title":"\u521d\u59cb\u5316 Operator"},{"location":"directx/project/#operator_3","text":"\u7ed1\u5b9a Temporary\u3001Persistent \u8d44\u6e90 \u4e3a binding table \u7ed1\u5b9a Inputs \u4e3a binding table \u7ed1\u5b9a InputsOutputs \u6267\u884c Operator","title":"\u6267\u884c Operator"},{"location":"directx/project/#_3","text":"\u56de\u8bfb\u7ed3\u679c DirectML API \u6587\u6863 \u21a9 https://docs.microsoft.com/en-us/windows/desktop/api/directml/nf-directml-idmlbindingtable-bindpersistentresource \u21a9 https://docs.microsoft.com/en-us/windows/desktop/api/directml/nf-directml-idmlbindingtable-bindoutputs \u21a9 https://docs.microsoft.com/en-us/windows/desktop/direct3d12/dml-binding#describe-and-bind-any-temporarypersistent-resources \u21a9 https://docs.microsoft.com/en-us/windows/desktop/api/directml/nf-directml-idmlbindingtable-bindinputs \u21a9","title":"\u7ed3\u675f"},{"location":"tensorrt/","text":"TensorRT \u5b89\u88c5\u8bf4\u660e Ubuntu \u4f7f\u7528 Nvidia Docker\u3002\u4ec5\u9700\u8981\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u82f1\u4f1f\u8fbe\u9a71\u52a8\u3001Nvidia Docker\uff0c\u5e76\u4ece\u4e91\u7aef\u62c9\u53d6\u955c\u50cf\u5373\u53ef Windows Prerequisite CUDA Toolkit (CUDA versions 9.0, 10.0, and 10.1 are supported) cuDNN (cuDNN version 7.5.0 is supported)","title":"TensorRT \u5b89\u88c5\u8bf4\u660e"},{"location":"tensorrt/#tensorrt","text":"","title":"TensorRT \u5b89\u88c5\u8bf4\u660e"},{"location":"tensorrt/#ubuntu","text":"\u4f7f\u7528 Nvidia Docker\u3002\u4ec5\u9700\u8981\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5\u82f1\u4f1f\u8fbe\u9a71\u52a8\u3001Nvidia Docker\uff0c\u5e76\u4ece\u4e91\u7aef\u62c9\u53d6\u955c\u50cf\u5373\u53ef","title":"Ubuntu"},{"location":"tensorrt/#windows","text":"","title":"Windows"},{"location":"tensorrt/#prerequisite","text":"CUDA Toolkit (CUDA versions 9.0, 10.0, and 10.1 are supported) cuDNN (cuDNN version 7.5.0 is supported)","title":"Prerequisite"},{"location":"tensorrt/project/","text":"\u6d4b\u8bd5 $ bin \\t rtexec.exe --deploy = data \\m nist \\m nist.prototxt --model = data \\m nist \\m nist.caffemodel --output = prob \u6d4b\u8bd5\u73af\u5883 \u786c\u4ef6 GeForce RTX 2080 i3-4160 CPU @ 3.60Hz 7.94 GB RAM \u8f6f\u4ef6 \u663e\u5361\u9a71\u52a8 430.39 Windows 10 version 1903 Cuda 10.0 cuDnn 7.5.0 TensorRT 5.1 \u5bf9\u6bd4\u7ed3\u679c \u6a21\u578b input batch fp32 fp16 int8 ResNet50 224x224 41 37ms 10ms 5.7ms cnn-10 256x256 31 41ms 24ms 8.7ms cnn-10 1080x1920 1 43ms 24ms 8.7ms rcnn-10 1080x1920 1 44ms 25ms 8.9ms \u7ed3\u679c\u7ec6\u8282 ResNet50 224x224x3 \u53d61000\u6b21\u63a8\u65ad\u5e73\u5747\u65f6\u95f4\uff0cbatch \u5927\u5c0f\u9009\u4e3a41\uff08224x224x41->1080p\uff09 fp32 ~37ms fp16 ~10ms int8 ~5.7ms &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --avgRuns=1000 [I] deploy: data\\resnet50\\ResNet50_N2.prototxt [I] model: data\\resnet50\\ResNet50_fp32.caffemodel [I] output: prob [I] batch: 41 [I] avgRuns: 1000 [I] Input \"data\": 3x224x224 [I] Output \"prob\": 1000x1x1 [I] Average over 1000 runs is 36.5989 ms (host walltime is 37.056 ms, 99% percentile time is 40.3437). [I] Average over 1000 runs is 36.8025 ms (host walltime is 37.323 ms, 99% percentile time is 40.1227). [I] Average over 1000 runs is 37.0368 ms (host walltime is 37.5535 ms, 99% percentile time is 40.8833). [I] Average over 1000 runs is 36.9303 ms (host walltime is 37.4497 ms, 99% percentile time is 40.2044). [I] Average over 1000 runs is 37.3102 ms (host walltime is 37.8786 ms, 99% percentile time is 41.175). [I] Average over 1000 runs is 37.4353 ms (host walltime is 37.8854 ms, 99% percentile time is 41.0356). [I] Average over 1000 runs is 37.217 ms (host walltime is 37.7247 ms, 99% percentile time is 38.4388). [I] Average over 1000 runs is 37.6249 ms (host walltime is 38.1419 ms, 99% percentile time is 41.4488). [I] Average over 1000 runs is 37.7456 ms (host walltime is 38.2448 ms, 99% percentile time is 41.5314). [I] Average over 1000 runs is 37.6224 ms (host walltime is 38.1146 ms, 99% percentile time is 41.2564). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --fp16 --avgRuns=1000 [I] deploy: data\\resnet50\\ResNet50_N2.prototxt [I] model: data\\resnet50\\ResNet50_fp32.caffemodel [I] output: prob [I] batch: 41 [I] fp16 [I] avgRuns: 1000 [I] Input \"data\": 3x224x224 [I] Output \"prob\": 1000x1x1 [I] Average over 1000 runs is 9.78154 ms (host walltime is 10.1434 ms, 99% percentile time is 10.9564). [I] Average over 1000 runs is 9.80061 ms (host walltime is 10.1679 ms, 99% percentile time is 10.8871). [I] Average over 1000 runs is 9.85407 ms (host walltime is 10.1652 ms, 99% percentile time is 11.1824). [I] Average over 1000 runs is 9.7751 ms (host walltime is 10.1061 ms, 99% percentile time is 10.3968). [I] Average over 1000 runs is 9.79208 ms (host walltime is 10.1128 ms, 99% percentile time is 10.3916). [I] Average over 1000 runs is 9.97582 ms (host walltime is 10.2224 ms, 99% percentile time is 11.2078). [I] Average over 1000 runs is 10.149 ms (host walltime is 10.419 ms, 99% percentile time is 11.2807). [I] Average over 1000 runs is 10.1625 ms (host walltime is 10.4829 ms, 99% percentile time is 11.2303). [I] Average over 1000 runs is 10.1419 ms (host walltime is 10.4095 ms, 99% percentile time is 11.2451). [I] Average over 1000 runs is 9.81545 ms (host walltime is 10.0941 ms, 99% percentile time is 10.4676). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --fp16 --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --int8 --avgRuns=1000 [I] deploy: data\\resnet50\\ResNet50_N2.prototxt [I] model: data\\resnet50\\ResNet50_fp32.caffemodel [I] output: prob [I] batch: 41 [I] int8 [I] avgRuns: 1000 [I] Input \"data\": 3x224x224 [I] Output \"prob\": 1000x1x1 [I] Average over 1000 runs is 5.74966 ms (host walltime is 6.04008 ms, 99% percentile time is 6.38528). [I] Average over 1000 runs is 5.71288 ms (host walltime is 5.9746 ms, 99% percentile time is 6.29187). [I] Average over 1000 runs is 5.78671 ms (host walltime is 6.10283 ms, 99% percentile time is 6.49056). [I] Average over 1000 runs is 5.74245 ms (host walltime is 6.01866 ms, 99% percentile time is 6.32483). [I] Average over 1000 runs is 5.7246 ms (host walltime is 5.96897 ms, 99% percentile time is 6.26262). [I] Average over 1000 runs is 5.71427 ms (host walltime is 5.94986 ms, 99% percentile time is 6.2911). [I] Average over 1000 runs is 5.76482 ms (host walltime is 6.01559 ms, 99% percentile time is 6.91024). [I] Average over 1000 runs is 5.79972 ms (host walltime is 6.10345 ms, 99% percentile time is 6.41738). [I] Average over 1000 runs is 5.80141 ms (host walltime is 6.10004 ms, 99% percentile time is 6.45424). [I] Average over 1000 runs is 5.75387 ms (host walltime is 6.00265 ms, 99% percentile time is 6.24486). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --int8 --avgRuns=1000 \u9879\u76ee\u6a21\u578b cnn-10 256x256x31 Project/pytorch-mono/model/model_256_256.onnx \u53d61000\u6b21\u63a8\u65ad\u5e73\u5747\u65f6\u95f4\uff0cbatch \u5927\u5c0f\u9009\u4e3a31\uff08256x256x31->1080p\uff09 fp32 ~41ms fp16 ~ &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 [I] onnx: data\\project\\model_256_256.onnx [I] batch: 31 [I] avgRuns: 1000 ---------------------------------------------------------------- Input filename: data\\project\\model_256_256.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 41.0329 ms (host walltime is 41.5606 ms, 99% percentile time is 45.5106). [I] Average over 1000 runs is 41.384 ms (host walltime is 41.9805 ms, 99% percentile time is 44.7077). [I] Average over 1000 runs is 41.7235 ms (host walltime is 42.2927 ms, 99% percentile time is 44.0291). [I] Average over 1000 runs is 42.318 ms (host walltime is 42.8769 ms, 99% percentile time is 46.7965). [I] Average over 1000 runs is 42.7134 ms (host walltime is 43.2284 ms, 99% percentile time is 47.8287). [I] Average over 1000 runs is 42.7245 ms (host walltime is 43.2779 ms, 99% percentile time is 47.6328). [I] Average over 1000 runs is 42.8418 ms (host walltime is 43.4213 ms, 99% percentile time is 47.8085). [I] Average over 1000 runs is 42.9501 ms (host walltime is 43.5097 ms, 99% percentile time is 46.7021). [I] Average over 1000 runs is 43.1244 ms (host walltime is 43.7388 ms, 99% percentile time is 46.721). [I] Average over 1000 runs is 43.3234 ms (host walltime is 43.8563 ms, 99% percentile time is 47.3295). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --fp16 [I] onnx: data\\project\\model_256_256.onnx [I] batch: 31 [I] avgRuns: 1000 [I] fp16 ---------------------------------------------------------------- Input filename: data\\project\\model_256_256.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 24.7055 ms (host walltime is 25.1759 ms, 99% percentile time is 27.4459). [I] Average over 1000 runs is 24.5021 ms (host walltime is 25.0236 ms, 99% percentile time is 27.277). [I] Average over 1000 runs is 24.6615 ms (host walltime is 25.1073 ms, 99% percentile time is 27.5085). [I] Average over 1000 runs is 24.8448 ms (host walltime is 25.3175 ms, 99% percentile time is 27.521). [I] Average over 1000 runs is 24.7442 ms (host walltime is 25.2699 ms, 99% percentile time is 27.5941). [I] Average over 1000 runs is 24.7661 ms (host walltime is 25.3317 ms, 99% percentile time is 27.9524). [I] Average over 1000 runs is 24.5799 ms (host walltime is 25.0773 ms, 99% percentile time is 28.3853). [I] Average over 1000 runs is 24.339 ms (host walltime is 24.8702 ms, 99% percentile time is 25.3111). [I] Average over 1000 runs is 24.6018 ms (host walltime is 25.1085 ms, 99% percentile time is 27.2757). [I] Average over 1000 runs is 24.373 ms (host walltime is 24.8947 ms, 99% percentile time is 26.6195). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --fp16 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --int8 [I] onnx: data\\project\\model_256_256.onnx [I] batch: 31 [I] avgRuns: 1000 [I] int8 ---------------------------------------------------------------- Input filename: data\\project\\model_256_256.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 8.73495 ms (host walltime is 9.20868 ms, 99% percentile time is 10.3488). [I] Average over 1000 runs is 8.64708 ms (host walltime is 9.09442 ms, 99% percentile time is 10.1545). [I] Average over 1000 runs is 8.61756 ms (host walltime is 9.1009 ms, 99% percentile time is 9.4536). [I] Average over 1000 runs is 8.65811 ms (host walltime is 9.18585 ms, 99% percentile time is 9.25504). [I] Average over 1000 runs is 8.6794 ms (host walltime is 9.17846 ms, 99% percentile time is 9.27763). [I] Average over 1000 runs is 8.70592 ms (host walltime is 9.21263 ms, 99% percentile time is 9.44774). [I] Average over 1000 runs is 8.73309 ms (host walltime is 9.21576 ms, 99% percentile time is 10.0148). [I] Average over 1000 runs is 8.84639 ms (host walltime is 9.22179 ms, 99% percentile time is 10.2569). [I] Average over 1000 runs is 8.81949 ms (host walltime is 9.20646 ms, 99% percentile time is 10.2398). [I] Average over 1000 runs is 8.67455 ms (host walltime is 9.11521 ms, 99% percentile time is 9.55434). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --int8 cnn-10 1080x1920x1 Project/pytorch-mono/model/model_256_256.onnx &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 [I] onnx: data\\project\\model_1080_1920.onnx [I] avgRuns: 1000 ---------------------------------------------------------------- Input filename: data\\project\\model_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 42.5376 ms (host walltime is 43.1446 ms, 99% percentile time is 46.0185). [I] Average over 1000 runs is 43.0747 ms (host walltime is 43.689 ms, 99% percentile time is 46.514). [I] Average over 1000 runs is 43.358 ms (host walltime is 44.1039 ms, 99% percentile time is 46.8438). [I] Average over 1000 runs is 43.4793 ms (host walltime is 44.1057 ms, 99% percentile time is 46.8652). [I] Average over 1000 runs is 43.5992 ms (host walltime is 44.2232 ms, 99% percentile time is 46.9056). [I] Average over 1000 runs is 43.5605 ms (host walltime is 44.0431 ms, 99% percentile time is 46.944). [I] Average over 1000 runs is 43.7356 ms (host walltime is 44.245 ms, 99% percentile time is 47.3206). [I] Average over 1000 runs is 43.8808 ms (host walltime is 44.4116 ms, 99% percentile time is 47.3298). [I] Average over 1000 runs is 43.9177 ms (host walltime is 44.4749 ms, 99% percentile time is 47.3356). [I] Average over 1000 runs is 43.9216 ms (host walltime is 44.4726 ms, 99% percentile time is 47.4481). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --fp16 [I] onnx: data\\project\\model_1080_1920.onnx [I] avgRuns: 1000 [I] fp16 ---------------------------------------------------------------- Input filename: data\\project\\model_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 23.8831 ms (host walltime is 24.1203 ms, 99% percentile time is 24.3608). [I] Average over 1000 runs is 24.4111 ms (host walltime is 24.6861 ms, 99% percentile time is 27.3841). [I] Average over 1000 runs is 24.2999 ms (host walltime is 24.5427 ms, 99% percentile time is 26.731). [I] Average over 1000 runs is 24.4412 ms (host walltime is 24.7081 ms, 99% percentile time is 27.0674). [I] Average over 1000 runs is 24.4237 ms (host walltime is 24.6792 ms, 99% percentile time is 26.8421). [I] Average over 1000 runs is 24.8017 ms (host walltime is 25.0704 ms, 99% percentile time is 27.8242). [I] Average over 1000 runs is 24.5303 ms (host walltime is 24.7996 ms, 99% percentile time is 26.7191). [I] Average over 1000 runs is 25.0661 ms (host walltime is 25.3921 ms, 99% percentile time is 27.5972). [I] Average over 1000 runs is 24.9305 ms (host walltime is 25.1841 ms, 99% percentile time is 27.854). [I] Average over 1000 runs is 24.7011 ms (host walltime is 24.9569 ms, 99% percentile time is 27.7975). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --fp16 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --int8 [I] onnx: data\\project\\model_1080_1920.onnx [I] avgRuns: 1000 [I] int8 ---------------------------------------------------------------- Input filename: data\\project\\model_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 8.66529 ms (host walltime is 9.21411 ms, 99% percentile time is 9.0624). [I] Average over 1000 runs is 8.73242 ms (host walltime is 9.309 ms, 99% percentile time is 9.12998). [I] Average over 1000 runs is 8.74873 ms (host walltime is 9.35212 ms, 99% percentile time is 9.2423). [I] Average over 1000 runs is 8.73695 ms (host walltime is 9.25817 ms, 99% percentile time is 9.19722). [I] Average over 1000 runs is 8.74951 ms (host walltime is 9.2688 ms, 99% percentile time is 9.21798). [I] Average over 1000 runs is 8.71137 ms (host walltime is 9.13181 ms, 99% percentile time is 9.40294). [I] Average over 1000 runs is 8.77244 ms (host walltime is 9.22304 ms, 99% percentile time is 10.645). [I] Average over 1000 runs is 8.75664 ms (host walltime is 9.22046 ms, 99% percentile time is 9.9672). [I] Average over 1000 runs is 8.84094 ms (host walltime is 9.32806 ms, 99% percentile time is 10.4063). [I] Average over 1000 runs is 8.95917 ms (host walltime is 9.36222 ms, 99% percentile time is 10.5293). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --int8 rcnn-10 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --fp16 [I] onnx: data\\project\\rcnn_1080_1920.onnx [I] avgRuns: 1000 [I] fp16 ---------------------------------------------------------------- Input filename: data\\project\\rcnn_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 24.7947 ms (host walltime is 25.4378 ms, 99% percentile time is 27.7331). [I] Average over 1000 runs is 24.7445 ms (host walltime is 25.388 ms, 99% percentile time is 26.7407). [I] Average over 1000 runs is 25.1315 ms (host walltime is 25.7714 ms, 99% percentile time is 28.4338). [I] Average over 1000 runs is 24.9105 ms (host walltime is 25.5266 ms, 99% percentile time is 27.814). [I] Average over 1000 runs is 24.8836 ms (host walltime is 25.5464 ms, 99% percentile time is 27.5691). [I] Average over 1000 runs is 25.1164 ms (host walltime is 25.7185 ms, 99% percentile time is 27.9678). [I] Average over 1000 runs is 25.2527 ms (host walltime is 25.9155 ms, 99% percentile time is 28.072). [I] Average over 1000 runs is 25.2182 ms (host walltime is 25.84 ms, 99% percentile time is 28.8976). [I] Average over 1000 runs is 25.1389 ms (host walltime is 25.6296 ms, 99% percentile time is 27.8299). [I] Average over 1000 runs is 24.8778 ms (host walltime is 25.3639 ms, 99% percentile time is 26.6261). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --fp16 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --int8 [I] onnx: data\\project\\rcnn_1080_1920.onnx [I] avgRuns: 1000 [I] int8 ---------------------------------------------------------------- Input filename: data\\project\\rcnn_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 8.74936 ms (host walltime is 8.99387 ms, 99% percentile time is 9.86048). [I] Average over 1000 runs is 8.84878 ms (host walltime is 9.1176 ms, 99% percentile time is 10.4693). [I] Average over 1000 runs is 8.97294 ms (host walltime is 9.42228 ms, 99% percentile time is 10.1783). [I] Average over 1000 runs is 9.02047 ms (host walltime is 9.62085 ms, 99% percentile time is 9.66451). [I] Average over 1000 runs is 9.04405 ms (host walltime is 9.64213 ms, 99% percentile time is 9.72186). [I] Average over 1000 runs is 9.08964 ms (host walltime is 9.65024 ms, 99% percentile time is 9.99885). [I] Average over 1000 runs is 8.94115 ms (host walltime is 9.25126 ms, 99% percentile time is 9.94397). [I] Average over 1000 runs is 9.0138 ms (host walltime is 9.43296 ms, 99% percentile time is 9.77478). [I] Average over 1000 runs is 9.08845 ms (host walltime is 9.70892 ms, 99% percentile time is 9.81402). [I] Average over 1000 runs is 9.10858 ms (host walltime is 9.70716 ms, 99% percentile time is 9.93792). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --int8 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 [I] onnx: data\\project\\rcnn_1080_1920.onnx [I] avgRuns: 1000 ---------------------------------------------------------------- Input filename: data\\project\\rcnn_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 43.781 ms (host walltime is 44.2806 ms, 99% percentile time is 47.6637). [I] Average over 1000 runs is 44.2414 ms (host walltime is 44.756 ms, 99% percentile time is 49.923). [I] Average over 1000 runs is 44.6497 ms (host walltime is 45.1768 ms, 99% percentile time is 51.5759). [I] Average over 1000 runs is 44.3383 ms (host walltime is 44.8596 ms, 99% percentile time is 49.2312). [I] Average over 1000 runs is 43.9716 ms (host walltime is 44.5007 ms, 99% percentile time is 48.3427). [I] Average over 1000 runs is 44.1964 ms (host walltime is 44.7082 ms, 99% percentile time is 47.5894). [I] Average over 1000 runs is 44.2688 ms (host walltime is 44.807 ms, 99% percentile time is 48.258). [I] Average over 1000 runs is 44.5854 ms (host walltime is 45.1112 ms, 99% percentile time is 49.9511). [I] Average over 1000 runs is 44.2579 ms (host walltime is 44.7664 ms, 99% percentile time is 50.2773). [I] Average over 1000 runs is 44.5676 ms (host walltime is 45.0882 ms, 99% percentile time is 50.0144). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000","title":"\u6d4b\u8bd5"},{"location":"tensorrt/project/#_1","text":"$ bin \\t rtexec.exe --deploy = data \\m nist \\m nist.prototxt --model = data \\m nist \\m nist.caffemodel --output = prob","title":"\u6d4b\u8bd5"},{"location":"tensorrt/project/#_2","text":"\u786c\u4ef6 GeForce RTX 2080 i3-4160 CPU @ 3.60Hz 7.94 GB RAM \u8f6f\u4ef6 \u663e\u5361\u9a71\u52a8 430.39 Windows 10 version 1903 Cuda 10.0 cuDnn 7.5.0 TensorRT 5.1","title":"\u6d4b\u8bd5\u73af\u5883"},{"location":"tensorrt/project/#_3","text":"\u6a21\u578b input batch fp32 fp16 int8 ResNet50 224x224 41 37ms 10ms 5.7ms cnn-10 256x256 31 41ms 24ms 8.7ms cnn-10 1080x1920 1 43ms 24ms 8.7ms rcnn-10 1080x1920 1 44ms 25ms 8.9ms","title":"\u5bf9\u6bd4\u7ed3\u679c"},{"location":"tensorrt/project/#_4","text":"","title":"\u7ed3\u679c\u7ec6\u8282"},{"location":"tensorrt/project/#resnet50","text":"","title":"ResNet50"},{"location":"tensorrt/project/#224x224x3","text":"\u53d61000\u6b21\u63a8\u65ad\u5e73\u5747\u65f6\u95f4\uff0cbatch \u5927\u5c0f\u9009\u4e3a41\uff08224x224x41->1080p\uff09 fp32 ~37ms fp16 ~10ms int8 ~5.7ms &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --avgRuns=1000 [I] deploy: data\\resnet50\\ResNet50_N2.prototxt [I] model: data\\resnet50\\ResNet50_fp32.caffemodel [I] output: prob [I] batch: 41 [I] avgRuns: 1000 [I] Input \"data\": 3x224x224 [I] Output \"prob\": 1000x1x1 [I] Average over 1000 runs is 36.5989 ms (host walltime is 37.056 ms, 99% percentile time is 40.3437). [I] Average over 1000 runs is 36.8025 ms (host walltime is 37.323 ms, 99% percentile time is 40.1227). [I] Average over 1000 runs is 37.0368 ms (host walltime is 37.5535 ms, 99% percentile time is 40.8833). [I] Average over 1000 runs is 36.9303 ms (host walltime is 37.4497 ms, 99% percentile time is 40.2044). [I] Average over 1000 runs is 37.3102 ms (host walltime is 37.8786 ms, 99% percentile time is 41.175). [I] Average over 1000 runs is 37.4353 ms (host walltime is 37.8854 ms, 99% percentile time is 41.0356). [I] Average over 1000 runs is 37.217 ms (host walltime is 37.7247 ms, 99% percentile time is 38.4388). [I] Average over 1000 runs is 37.6249 ms (host walltime is 38.1419 ms, 99% percentile time is 41.4488). [I] Average over 1000 runs is 37.7456 ms (host walltime is 38.2448 ms, 99% percentile time is 41.5314). [I] Average over 1000 runs is 37.6224 ms (host walltime is 38.1146 ms, 99% percentile time is 41.2564). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --fp16 --avgRuns=1000 [I] deploy: data\\resnet50\\ResNet50_N2.prototxt [I] model: data\\resnet50\\ResNet50_fp32.caffemodel [I] output: prob [I] batch: 41 [I] fp16 [I] avgRuns: 1000 [I] Input \"data\": 3x224x224 [I] Output \"prob\": 1000x1x1 [I] Average over 1000 runs is 9.78154 ms (host walltime is 10.1434 ms, 99% percentile time is 10.9564). [I] Average over 1000 runs is 9.80061 ms (host walltime is 10.1679 ms, 99% percentile time is 10.8871). [I] Average over 1000 runs is 9.85407 ms (host walltime is 10.1652 ms, 99% percentile time is 11.1824). [I] Average over 1000 runs is 9.7751 ms (host walltime is 10.1061 ms, 99% percentile time is 10.3968). [I] Average over 1000 runs is 9.79208 ms (host walltime is 10.1128 ms, 99% percentile time is 10.3916). [I] Average over 1000 runs is 9.97582 ms (host walltime is 10.2224 ms, 99% percentile time is 11.2078). [I] Average over 1000 runs is 10.149 ms (host walltime is 10.419 ms, 99% percentile time is 11.2807). [I] Average over 1000 runs is 10.1625 ms (host walltime is 10.4829 ms, 99% percentile time is 11.2303). [I] Average over 1000 runs is 10.1419 ms (host walltime is 10.4095 ms, 99% percentile time is 11.2451). [I] Average over 1000 runs is 9.81545 ms (host walltime is 10.0941 ms, 99% percentile time is 10.4676). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --fp16 --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --int8 --avgRuns=1000 [I] deploy: data\\resnet50\\ResNet50_N2.prototxt [I] model: data\\resnet50\\ResNet50_fp32.caffemodel [I] output: prob [I] batch: 41 [I] int8 [I] avgRuns: 1000 [I] Input \"data\": 3x224x224 [I] Output \"prob\": 1000x1x1 [I] Average over 1000 runs is 5.74966 ms (host walltime is 6.04008 ms, 99% percentile time is 6.38528). [I] Average over 1000 runs is 5.71288 ms (host walltime is 5.9746 ms, 99% percentile time is 6.29187). [I] Average over 1000 runs is 5.78671 ms (host walltime is 6.10283 ms, 99% percentile time is 6.49056). [I] Average over 1000 runs is 5.74245 ms (host walltime is 6.01866 ms, 99% percentile time is 6.32483). [I] Average over 1000 runs is 5.7246 ms (host walltime is 5.96897 ms, 99% percentile time is 6.26262). [I] Average over 1000 runs is 5.71427 ms (host walltime is 5.94986 ms, 99% percentile time is 6.2911). [I] Average over 1000 runs is 5.76482 ms (host walltime is 6.01559 ms, 99% percentile time is 6.91024). [I] Average over 1000 runs is 5.79972 ms (host walltime is 6.10345 ms, 99% percentile time is 6.41738). [I] Average over 1000 runs is 5.80141 ms (host walltime is 6.10004 ms, 99% percentile time is 6.45424). [I] Average over 1000 runs is 5.75387 ms (host walltime is 6.00265 ms, 99% percentile time is 6.24486). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --deploy=data\\resnet50\\ResNet50_N2.prototxt --model=data\\resnet50\\ResNet50_fp32.caffemodel --output=prob --batch=41 --int8 --avgRuns=1000","title":"224x224x3"},{"location":"tensorrt/project/#_5","text":"","title":"\u9879\u76ee\u6a21\u578b"},{"location":"tensorrt/project/#cnn-10-256x256x31","text":"Project/pytorch-mono/model/model_256_256.onnx \u53d61000\u6b21\u63a8\u65ad\u5e73\u5747\u65f6\u95f4\uff0cbatch \u5927\u5c0f\u9009\u4e3a31\uff08256x256x31->1080p\uff09 fp32 ~41ms fp16 ~ &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 [I] onnx: data\\project\\model_256_256.onnx [I] batch: 31 [I] avgRuns: 1000 ---------------------------------------------------------------- Input filename: data\\project\\model_256_256.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 41.0329 ms (host walltime is 41.5606 ms, 99% percentile time is 45.5106). [I] Average over 1000 runs is 41.384 ms (host walltime is 41.9805 ms, 99% percentile time is 44.7077). [I] Average over 1000 runs is 41.7235 ms (host walltime is 42.2927 ms, 99% percentile time is 44.0291). [I] Average over 1000 runs is 42.318 ms (host walltime is 42.8769 ms, 99% percentile time is 46.7965). [I] Average over 1000 runs is 42.7134 ms (host walltime is 43.2284 ms, 99% percentile time is 47.8287). [I] Average over 1000 runs is 42.7245 ms (host walltime is 43.2779 ms, 99% percentile time is 47.6328). [I] Average over 1000 runs is 42.8418 ms (host walltime is 43.4213 ms, 99% percentile time is 47.8085). [I] Average over 1000 runs is 42.9501 ms (host walltime is 43.5097 ms, 99% percentile time is 46.7021). [I] Average over 1000 runs is 43.1244 ms (host walltime is 43.7388 ms, 99% percentile time is 46.721). [I] Average over 1000 runs is 43.3234 ms (host walltime is 43.8563 ms, 99% percentile time is 47.3295). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --fp16 [I] onnx: data\\project\\model_256_256.onnx [I] batch: 31 [I] avgRuns: 1000 [I] fp16 ---------------------------------------------------------------- Input filename: data\\project\\model_256_256.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 24.7055 ms (host walltime is 25.1759 ms, 99% percentile time is 27.4459). [I] Average over 1000 runs is 24.5021 ms (host walltime is 25.0236 ms, 99% percentile time is 27.277). [I] Average over 1000 runs is 24.6615 ms (host walltime is 25.1073 ms, 99% percentile time is 27.5085). [I] Average over 1000 runs is 24.8448 ms (host walltime is 25.3175 ms, 99% percentile time is 27.521). [I] Average over 1000 runs is 24.7442 ms (host walltime is 25.2699 ms, 99% percentile time is 27.5941). [I] Average over 1000 runs is 24.7661 ms (host walltime is 25.3317 ms, 99% percentile time is 27.9524). [I] Average over 1000 runs is 24.5799 ms (host walltime is 25.0773 ms, 99% percentile time is 28.3853). [I] Average over 1000 runs is 24.339 ms (host walltime is 24.8702 ms, 99% percentile time is 25.3111). [I] Average over 1000 runs is 24.6018 ms (host walltime is 25.1085 ms, 99% percentile time is 27.2757). [I] Average over 1000 runs is 24.373 ms (host walltime is 24.8947 ms, 99% percentile time is 26.6195). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --fp16 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --int8 [I] onnx: data\\project\\model_256_256.onnx [I] batch: 31 [I] avgRuns: 1000 [I] int8 ---------------------------------------------------------------- Input filename: data\\project\\model_256_256.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 8.73495 ms (host walltime is 9.20868 ms, 99% percentile time is 10.3488). [I] Average over 1000 runs is 8.64708 ms (host walltime is 9.09442 ms, 99% percentile time is 10.1545). [I] Average over 1000 runs is 8.61756 ms (host walltime is 9.1009 ms, 99% percentile time is 9.4536). [I] Average over 1000 runs is 8.65811 ms (host walltime is 9.18585 ms, 99% percentile time is 9.25504). [I] Average over 1000 runs is 8.6794 ms (host walltime is 9.17846 ms, 99% percentile time is 9.27763). [I] Average over 1000 runs is 8.70592 ms (host walltime is 9.21263 ms, 99% percentile time is 9.44774). [I] Average over 1000 runs is 8.73309 ms (host walltime is 9.21576 ms, 99% percentile time is 10.0148). [I] Average over 1000 runs is 8.84639 ms (host walltime is 9.22179 ms, 99% percentile time is 10.2569). [I] Average over 1000 runs is 8.81949 ms (host walltime is 9.20646 ms, 99% percentile time is 10.2398). [I] Average over 1000 runs is 8.67455 ms (host walltime is 9.11521 ms, 99% percentile time is 9.55434). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_256_256.onnx --batch=31 --avgRuns=1000 --int8","title":"cnn-10 256x256x31"},{"location":"tensorrt/project/#cnn-10-1080x1920x1","text":"Project/pytorch-mono/model/model_256_256.onnx &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 [I] onnx: data\\project\\model_1080_1920.onnx [I] avgRuns: 1000 ---------------------------------------------------------------- Input filename: data\\project\\model_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 42.5376 ms (host walltime is 43.1446 ms, 99% percentile time is 46.0185). [I] Average over 1000 runs is 43.0747 ms (host walltime is 43.689 ms, 99% percentile time is 46.514). [I] Average over 1000 runs is 43.358 ms (host walltime is 44.1039 ms, 99% percentile time is 46.8438). [I] Average over 1000 runs is 43.4793 ms (host walltime is 44.1057 ms, 99% percentile time is 46.8652). [I] Average over 1000 runs is 43.5992 ms (host walltime is 44.2232 ms, 99% percentile time is 46.9056). [I] Average over 1000 runs is 43.5605 ms (host walltime is 44.0431 ms, 99% percentile time is 46.944). [I] Average over 1000 runs is 43.7356 ms (host walltime is 44.245 ms, 99% percentile time is 47.3206). [I] Average over 1000 runs is 43.8808 ms (host walltime is 44.4116 ms, 99% percentile time is 47.3298). [I] Average over 1000 runs is 43.9177 ms (host walltime is 44.4749 ms, 99% percentile time is 47.3356). [I] Average over 1000 runs is 43.9216 ms (host walltime is 44.4726 ms, 99% percentile time is 47.4481). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --fp16 [I] onnx: data\\project\\model_1080_1920.onnx [I] avgRuns: 1000 [I] fp16 ---------------------------------------------------------------- Input filename: data\\project\\model_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 23.8831 ms (host walltime is 24.1203 ms, 99% percentile time is 24.3608). [I] Average over 1000 runs is 24.4111 ms (host walltime is 24.6861 ms, 99% percentile time is 27.3841). [I] Average over 1000 runs is 24.2999 ms (host walltime is 24.5427 ms, 99% percentile time is 26.731). [I] Average over 1000 runs is 24.4412 ms (host walltime is 24.7081 ms, 99% percentile time is 27.0674). [I] Average over 1000 runs is 24.4237 ms (host walltime is 24.6792 ms, 99% percentile time is 26.8421). [I] Average over 1000 runs is 24.8017 ms (host walltime is 25.0704 ms, 99% percentile time is 27.8242). [I] Average over 1000 runs is 24.5303 ms (host walltime is 24.7996 ms, 99% percentile time is 26.7191). [I] Average over 1000 runs is 25.0661 ms (host walltime is 25.3921 ms, 99% percentile time is 27.5972). [I] Average over 1000 runs is 24.9305 ms (host walltime is 25.1841 ms, 99% percentile time is 27.854). [I] Average over 1000 runs is 24.7011 ms (host walltime is 24.9569 ms, 99% percentile time is 27.7975). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --fp16 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --int8 [I] onnx: data\\project\\model_1080_1920.onnx [I] avgRuns: 1000 [I] int8 ---------------------------------------------------------------- Input filename: data\\project\\model_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 8.66529 ms (host walltime is 9.21411 ms, 99% percentile time is 9.0624). [I] Average over 1000 runs is 8.73242 ms (host walltime is 9.309 ms, 99% percentile time is 9.12998). [I] Average over 1000 runs is 8.74873 ms (host walltime is 9.35212 ms, 99% percentile time is 9.2423). [I] Average over 1000 runs is 8.73695 ms (host walltime is 9.25817 ms, 99% percentile time is 9.19722). [I] Average over 1000 runs is 8.74951 ms (host walltime is 9.2688 ms, 99% percentile time is 9.21798). [I] Average over 1000 runs is 8.71137 ms (host walltime is 9.13181 ms, 99% percentile time is 9.40294). [I] Average over 1000 runs is 8.77244 ms (host walltime is 9.22304 ms, 99% percentile time is 10.645). [I] Average over 1000 runs is 8.75664 ms (host walltime is 9.22046 ms, 99% percentile time is 9.9672). [I] Average over 1000 runs is 8.84094 ms (host walltime is 9.32806 ms, 99% percentile time is 10.4063). [I] Average over 1000 runs is 8.95917 ms (host walltime is 9.36222 ms, 99% percentile time is 10.5293). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\model_1080_1920.onnx --avgRuns=1000 --int8","title":"cnn-10 1080x1920x1"},{"location":"tensorrt/project/#rcnn-10","text":"&&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --fp16 [I] onnx: data\\project\\rcnn_1080_1920.onnx [I] avgRuns: 1000 [I] fp16 ---------------------------------------------------------------- Input filename: data\\project\\rcnn_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 24.7947 ms (host walltime is 25.4378 ms, 99% percentile time is 27.7331). [I] Average over 1000 runs is 24.7445 ms (host walltime is 25.388 ms, 99% percentile time is 26.7407). [I] Average over 1000 runs is 25.1315 ms (host walltime is 25.7714 ms, 99% percentile time is 28.4338). [I] Average over 1000 runs is 24.9105 ms (host walltime is 25.5266 ms, 99% percentile time is 27.814). [I] Average over 1000 runs is 24.8836 ms (host walltime is 25.5464 ms, 99% percentile time is 27.5691). [I] Average over 1000 runs is 25.1164 ms (host walltime is 25.7185 ms, 99% percentile time is 27.9678). [I] Average over 1000 runs is 25.2527 ms (host walltime is 25.9155 ms, 99% percentile time is 28.072). [I] Average over 1000 runs is 25.2182 ms (host walltime is 25.84 ms, 99% percentile time is 28.8976). [I] Average over 1000 runs is 25.1389 ms (host walltime is 25.6296 ms, 99% percentile time is 27.8299). [I] Average over 1000 runs is 24.8778 ms (host walltime is 25.3639 ms, 99% percentile time is 26.6261). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --fp16 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --int8 [I] onnx: data\\project\\rcnn_1080_1920.onnx [I] avgRuns: 1000 [I] int8 ---------------------------------------------------------------- Input filename: data\\project\\rcnn_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 8.74936 ms (host walltime is 8.99387 ms, 99% percentile time is 9.86048). [I] Average over 1000 runs is 8.84878 ms (host walltime is 9.1176 ms, 99% percentile time is 10.4693). [I] Average over 1000 runs is 8.97294 ms (host walltime is 9.42228 ms, 99% percentile time is 10.1783). [I] Average over 1000 runs is 9.02047 ms (host walltime is 9.62085 ms, 99% percentile time is 9.66451). [I] Average over 1000 runs is 9.04405 ms (host walltime is 9.64213 ms, 99% percentile time is 9.72186). [I] Average over 1000 runs is 9.08964 ms (host walltime is 9.65024 ms, 99% percentile time is 9.99885). [I] Average over 1000 runs is 8.94115 ms (host walltime is 9.25126 ms, 99% percentile time is 9.94397). [I] Average over 1000 runs is 9.0138 ms (host walltime is 9.43296 ms, 99% percentile time is 9.77478). [I] Average over 1000 runs is 9.08845 ms (host walltime is 9.70892 ms, 99% percentile time is 9.81402). [I] Average over 1000 runs is 9.10858 ms (host walltime is 9.70716 ms, 99% percentile time is 9.93792). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 --int8 &&&& RUNNING TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000 [I] onnx: data\\project\\rcnn_1080_1920.onnx [I] avgRuns: 1000 ---------------------------------------------------------------- Input filename: data\\project\\rcnn_1080_1920.onnx ONNX IR version: 0.0.3 Opset version: 9 Producer name: pytorch Producer version: 0.4 Domain: Model version: 0 Doc string: ---------------------------------------------------------------- [I] Average over 1000 runs is 43.781 ms (host walltime is 44.2806 ms, 99% percentile time is 47.6637). [I] Average over 1000 runs is 44.2414 ms (host walltime is 44.756 ms, 99% percentile time is 49.923). [I] Average over 1000 runs is 44.6497 ms (host walltime is 45.1768 ms, 99% percentile time is 51.5759). [I] Average over 1000 runs is 44.3383 ms (host walltime is 44.8596 ms, 99% percentile time is 49.2312). [I] Average over 1000 runs is 43.9716 ms (host walltime is 44.5007 ms, 99% percentile time is 48.3427). [I] Average over 1000 runs is 44.1964 ms (host walltime is 44.7082 ms, 99% percentile time is 47.5894). [I] Average over 1000 runs is 44.2688 ms (host walltime is 44.807 ms, 99% percentile time is 48.258). [I] Average over 1000 runs is 44.5854 ms (host walltime is 45.1112 ms, 99% percentile time is 49.9511). [I] Average over 1000 runs is 44.2579 ms (host walltime is 44.7664 ms, 99% percentile time is 50.2773). [I] Average over 1000 runs is 44.5676 ms (host walltime is 45.0882 ms, 99% percentile time is 50.0144). &&&& PASSED TensorRT.trtexec # bin\\trtexec.exe --onnx=data\\project\\rcnn_1080_1920.onnx --avgRuns=1000","title":"rcnn-10"},{"location":"tensorrt/trt-int8/","text":"TensorRT INT8 TensorRT \u652f\u6301\u8f6c\u6362 int8 \u6a21\u578b\u3001int8 \u63a8\u65ad\u529f\u80fd\u3002TensorRT \u635f\u89c6\u5c11\u91cf\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u5c06 fp32 \u6a21\u578b\u8f6c\u6362\u4e3a int8\uff0c\u5927\u5e45\u5ea6\u51cf\u5c11\u63a8\u65ad\u65f6\u95f4\uff0c\u51cf\u5c11\u5b58\u50a8\u6d88\u8017\uff0c\u63d0\u5347\u541e\u5410\u91cf\u3002 \u4e0b\u56fe\u4e3a\u8f6c\u6362\u540e\u7684\u7cbe\u5ea6\u5bf9\u6bd4 TensorRT INT8 \u63a8\u65ad\u5177\u4f53\u6b65\u9aa4 \u7a0b\u5e8f\u63d0\u4f9b\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6 TensorRT \u5229\u7528\u6570\u636e\u96c6\u751f\u6210\u6821\u51c6\u8868 \u4fdd\u5b58\u6821\u51c6\u8868","title":"TensorRT INT8"},{"location":"tensorrt/trt-int8/#tensorrt-int8","text":"TensorRT \u652f\u6301\u8f6c\u6362 int8 \u6a21\u578b\u3001int8 \u63a8\u65ad\u529f\u80fd\u3002TensorRT \u635f\u89c6\u5c11\u91cf\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u5c06 fp32 \u6a21\u578b\u8f6c\u6362\u4e3a int8\uff0c\u5927\u5e45\u5ea6\u51cf\u5c11\u63a8\u65ad\u65f6\u95f4\uff0c\u51cf\u5c11\u5b58\u50a8\u6d88\u8017\uff0c\u63d0\u5347\u541e\u5410\u91cf\u3002 \u4e0b\u56fe\u4e3a\u8f6c\u6362\u540e\u7684\u7cbe\u5ea6\u5bf9\u6bd4 TensorRT INT8 \u63a8\u65ad\u5177\u4f53\u6b65\u9aa4 \u7a0b\u5e8f\u63d0\u4f9b\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6 TensorRT \u5229\u7528\u6570\u636e\u96c6\u751f\u6210\u6821\u51c6\u8868 \u4fdd\u5b58\u6821\u51c6\u8868","title":"TensorRT INT8"},{"location":"tensorrt/trtexec/","text":"trtexec \u6e90\u7801\u5206\u6790 \u6e90\u7801\u6765\u81ea\u82f1\u4f1f\u8fbe\u5b98\u65b9\u6837\u4f8b\uff0c\u7528\u4e8e benchmark \u6d4b\u8bd5\u3002\u7ed9\u5b9a caffe/onnx/uff \u683c\u5f0f\u6a21\u578b\uff0c\u5728\u968f\u673a\u6570\u636e\u4e0a\u6d4b\u8bd5 Inference \u6548\u7387\uff0c\u6b64\u5916\u53ef\u4ee5\u4f5c\u4e3a\u751f\u6210\u5e8f\u5217\u5316 engine \u7684\u6837\u4f8b\u3002 \u4f7f\u7528\u8bf4\u660e\u5982\u4e0b\uff1a Mandatory params: --deploy=<file> Caffe deploy file OR --uff=<file> UFF file OR --onnx=<file> ONNX Model file OR --loadEngine=<file> Load a saved engine Mandatory params for UFF: --uffInput=<name>,C,H,W Input blob name and its dimensions for UFF parser (can be specified multiple times) --output=<name> Output blob name (can be specified multiple times) Mandatory params for Caffe: --output=<name> Output blob name (can be specified multiple times) Optional params: --model=<file> Caffe model file (default = no model, random weights used) --batch=N Set batch size (default = 1) --device=N Set cuda device to N (default = 0) --iterations=N Run N iterations (default = 10) --avgRuns=N Set avgRuns to N - perf is measured as an average of avgRuns (default=10) --percentile=P For each iteration, report the percentile time at P percentage (0<=P<=100, with 0 representing min, and 100 representing max; default = 99.0%) --workspace=N Set workspace size in megabytes (default = 16) --fp16 Run in fp16 mode (default = false). Permits 16-bit kernels --int8 Run in int8 mode (default = false). Currently no support for ONNX model. --verbose Use verbose logging (default = false) --saveEngine=<file> Save a serialized engine to file. --loadEngine=<file> Load a serialized engine from file. --calib=<file> Read INT8 calibration cache file. Currently no support for ONNX model. --useDLACore=N Specify a DLA engine for layers that support DLA. Value can range from 0 to n-1, where n is the number of DLA engines on the platform. --allowGPUFallback If --useDLACore flag is present and if a layer can't run on DLA, then run on GPU. --useSpinWait Actively wait for work completion. This option may decrease multi-process synchronization time at the cost of additional CPU usage. (default = false) --dumpOutput Dump outputs at end of test. -h, --help Print usage \u4f8b\u5b50\uff1a $ trtexec --deploy = /path/to/mnist.prototxt --model = /path/to/mnist.caffemodel --output = prob main \u65b9\u6cd5 int main ( int argc , char ** argv ) { // create a TensorRT model from the caffe/uff/onnx model and serialize it to a stream auto sampleTest = gLogger . defineTest ( gSampleName , argc , const_cast < const char **> ( argv )); gLogger . reportTestStart ( sampleTest ); if ( ! parseArgs ( argc , argv )) { return gLogger . reportFail ( sampleTest ); } if ( gParams . help ) { printUsage (); return gLogger . reportPass ( sampleTest ); } if ( gParams . verbose ) { setReportableSeverity ( Severity :: kVERBOSE ); } cudaSetDevice ( gParams . device ); initLibNvInferPlugins ( & gLogger . getTRTLogger (), \"\" ); ICudaEngine * engine = createEngine (); if ( ! engine ) { gLogError << \"Engine could not be created\" << std :: endl ; return gLogger . reportFail ( sampleTest ); } if ( gParams . uffFile . empty () && gParams . onnxModelFile . empty ()) { nvcaffeparser1 :: shutdownProtobufLibrary (); } else if ( gParams . deployFile . empty () && gParams . onnxModelFile . empty ()) { nvuffparser :: shutdownProtobufLibrary (); } doInference ( * engine ); engine -> destroy (); return gLogger . reportPass ( sampleTest ); } \u4e3b\u51fd\u6570\u5b8c\u6210\u4ee5\u4e0b\u5de5\u4f5c\uff1a \u89e3\u6790\u53c2\u6570 createEngine doInference \u63a5\u4e0b\u6765\u5177\u4f53\u5206\u6790 createEngine \u548c doInference \u4e24\u4e2a\u51fd\u6570 createEngine static ICudaEngine * createEngine () { ICudaEngine * engine ; // load directly from serialized engine file if deploy not specified if ( ! gParams . loadEngine . empty ()) { std :: vector < char > trtModelStream ; size_t size { 0 }; std :: ifstream file ( gParams . loadEngine , std :: ios :: binary ); if ( file . good ()) { file . seekg ( 0 , file . end ); size = file . tellg (); file . seekg ( 0 , file . beg ); trtModelStream . resize ( size ); file . read ( trtModelStream . data (), size ); file . close (); } IRuntime * infer = createInferRuntime ( gLogger . getTRTLogger ()); if ( gParams . useDLACore >= 0 ) { infer -> setDLACore ( gParams . useDLACore ); } engine = infer -> deserializeCudaEngine ( trtModelStream . data (), size , nullptr ); gLogInfo << gParams . loadEngine << \" has been successfully loaded.\" << std :: endl ; infer -> destroy (); return engine ; } if (( ! gParams . deployFile . empty ()) || ( ! gParams . uffFile . empty ()) || ( ! gParams . onnxModelFile . empty ())) { if ( ! gParams . uffFile . empty ()) { engine = uffToTRTModel (); } else if ( ! gParams . onnxModelFile . empty ()) { engine = onnxToTRTModel (); } else { engine = caffeToTRTModel (); } if ( ! engine ) { gLogError << \"Engine could not be created\" << std :: endl ; return nullptr ; } if ( ! gParams . saveEngine . empty ()) { std :: ofstream p ( gParams . saveEngine , std :: ios :: binary ); if ( ! p ) { gLogError << \"could not open plan output file\" << std :: endl ; return nullptr ; } IHostMemory * ptr = engine -> serialize (); if ( ptr == nullptr ) { gLogError << \"could not serialize engine.\" << std :: endl ; return nullptr ; } p . write ( reinterpret_cast < const char *> ( ptr -> data ()), ptr -> size ()); ptr -> destroy (); gLogInfo << \"Engine has been successfully saved to \" << gParams . saveEngine << std :: endl ; } return engine ; } // complain about empty deploy file gLogError << \"Deploy file not specified\" << std :: endl ; return nullptr ; } \u5c06 caffe/onnx/uff \u6a21\u578b\u8f6c\u6362\u4e3a TensorRT engine \u5e8f\u5217\u5316 engine doInference void doInference ( ICudaEngine & engine ) { IExecutionContext * context = engine . createExecutionContext (); // Use an aliasing shared_ptr since we don't want engine to be deleted when bufferManager goes out of scope. std :: shared_ptr < ICudaEngine > emptyPtr {}; std :: shared_ptr < ICudaEngine > aliasPtr ( emptyPtr , & engine ); samplesCommon :: BufferManager bufferManager ( aliasPtr , gParams . batchSize ); std :: vector < void *> buffers = bufferManager . getDeviceBindings (); cudaStream_t stream ; CHECK ( cudaStreamCreate ( & stream )); cudaEvent_t start , end ; unsigned int cudaEventFlags = gParams . useSpinWait ? cudaEventDefault : cudaEventBlockingSync ; CHECK ( cudaEventCreateWithFlags ( & start , cudaEventFlags )); CHECK ( cudaEventCreateWithFlags ( & end , cudaEventFlags )); std :: vector < float > times ( gParams . avgRuns ); for ( int j = 0 ; j < gParams . iterations ; j ++ ) { float totalGpu { 0 }, totalHost { 0 }; // GPU and Host timers for ( int i = 0 ; i < gParams . avgRuns ; i ++ ) { auto tStart = std :: chrono :: high_resolution_clock :: now (); cudaEventRecord ( start , stream ); context -> enqueue ( gParams . batchSize , & buffers [ 0 ], stream , nullptr ); cudaEventRecord ( end , stream ); cudaEventSynchronize ( end ); auto tEnd = std :: chrono :: high_resolution_clock :: now (); totalHost += std :: chrono :: duration < float , std :: milli > ( tEnd - tStart ). count (); float ms ; cudaEventElapsedTime ( & ms , start , end ); times [ i ] = ms ; totalGpu += ms ; } totalGpu /= gParams . avgRuns ; totalHost /= gParams . avgRuns ; gLogInfo << \"Average over \" << gParams . avgRuns << \" runs is \" << totalGpu << \" ms (host walltime is \" << totalHost << \" ms, \" << static_cast < int > ( gParams . pct ) << \"\\% percentile time is \" << percentile ( gParams . pct , times ) << \").\" << std :: endl ; } if ( gParams . dumpOutput ) { bufferManager . copyOutputToHost (); int nbBindings = engine . getNbBindings (); for ( int i = 0 ; i < nbBindings ; i ++ ) { if ( ! engine . bindingIsInput ( i )) { const char * tensorName = engine . getBindingName ( i ); gLogInfo << \"Dumping output tensor \" << tensorName << \":\" << std :: endl ; bufferManager . dumpBuffer ( gLogInfo , tensorName ); } } } cudaStreamDestroy ( stream ); cudaEventDestroy ( start ); cudaEventDestroy ( end ); context -> destroy (); } \u8fed\u4ee3\u591a\u6b21\uff0c\u5206\u522b\u8ba1\u7b97\u5728 GPU \u4e0a\u8fd0\u7b97\u7684\u65f6\u95f4\uff0c\u4ee5\u53ca\u4ece\u5f00\u59cb\u5230\u5b8c\u6210\u8fdb\u884c\u7684 walltime\uff0c\u5305\u62ec\u5176\u4ed6\u8fdb\u7a0b\u4f7f\u7528\u7684\u65f6\u95f4\u548c\u672c\u8fdb\u7a0b\u8017\u8d39\u5728\u963b\u585e\uff08\u5982\u7b49\u5f85I/O\u64cd\u4f5c\u5b8c\u6210\uff09\u4e0a\u7684\u65f6\u95f4\u3002 \u9500\u6bc1\u8d44\u6e90","title":"trtexec \u6e90\u7801\u5206\u6790"},{"location":"tensorrt/trtexec/#trtexec","text":"\u6e90\u7801\u6765\u81ea\u82f1\u4f1f\u8fbe\u5b98\u65b9\u6837\u4f8b\uff0c\u7528\u4e8e benchmark \u6d4b\u8bd5\u3002\u7ed9\u5b9a caffe/onnx/uff \u683c\u5f0f\u6a21\u578b\uff0c\u5728\u968f\u673a\u6570\u636e\u4e0a\u6d4b\u8bd5 Inference \u6548\u7387\uff0c\u6b64\u5916\u53ef\u4ee5\u4f5c\u4e3a\u751f\u6210\u5e8f\u5217\u5316 engine \u7684\u6837\u4f8b\u3002 \u4f7f\u7528\u8bf4\u660e\u5982\u4e0b\uff1a Mandatory params: --deploy=<file> Caffe deploy file OR --uff=<file> UFF file OR --onnx=<file> ONNX Model file OR --loadEngine=<file> Load a saved engine Mandatory params for UFF: --uffInput=<name>,C,H,W Input blob name and its dimensions for UFF parser (can be specified multiple times) --output=<name> Output blob name (can be specified multiple times) Mandatory params for Caffe: --output=<name> Output blob name (can be specified multiple times) Optional params: --model=<file> Caffe model file (default = no model, random weights used) --batch=N Set batch size (default = 1) --device=N Set cuda device to N (default = 0) --iterations=N Run N iterations (default = 10) --avgRuns=N Set avgRuns to N - perf is measured as an average of avgRuns (default=10) --percentile=P For each iteration, report the percentile time at P percentage (0<=P<=100, with 0 representing min, and 100 representing max; default = 99.0%) --workspace=N Set workspace size in megabytes (default = 16) --fp16 Run in fp16 mode (default = false). Permits 16-bit kernels --int8 Run in int8 mode (default = false). Currently no support for ONNX model. --verbose Use verbose logging (default = false) --saveEngine=<file> Save a serialized engine to file. --loadEngine=<file> Load a serialized engine from file. --calib=<file> Read INT8 calibration cache file. Currently no support for ONNX model. --useDLACore=N Specify a DLA engine for layers that support DLA. Value can range from 0 to n-1, where n is the number of DLA engines on the platform. --allowGPUFallback If --useDLACore flag is present and if a layer can't run on DLA, then run on GPU. --useSpinWait Actively wait for work completion. This option may decrease multi-process synchronization time at the cost of additional CPU usage. (default = false) --dumpOutput Dump outputs at end of test. -h, --help Print usage \u4f8b\u5b50\uff1a $ trtexec --deploy = /path/to/mnist.prototxt --model = /path/to/mnist.caffemodel --output = prob","title":"trtexec \u6e90\u7801\u5206\u6790"},{"location":"tensorrt/trtexec/#main","text":"int main ( int argc , char ** argv ) { // create a TensorRT model from the caffe/uff/onnx model and serialize it to a stream auto sampleTest = gLogger . defineTest ( gSampleName , argc , const_cast < const char **> ( argv )); gLogger . reportTestStart ( sampleTest ); if ( ! parseArgs ( argc , argv )) { return gLogger . reportFail ( sampleTest ); } if ( gParams . help ) { printUsage (); return gLogger . reportPass ( sampleTest ); } if ( gParams . verbose ) { setReportableSeverity ( Severity :: kVERBOSE ); } cudaSetDevice ( gParams . device ); initLibNvInferPlugins ( & gLogger . getTRTLogger (), \"\" ); ICudaEngine * engine = createEngine (); if ( ! engine ) { gLogError << \"Engine could not be created\" << std :: endl ; return gLogger . reportFail ( sampleTest ); } if ( gParams . uffFile . empty () && gParams . onnxModelFile . empty ()) { nvcaffeparser1 :: shutdownProtobufLibrary (); } else if ( gParams . deployFile . empty () && gParams . onnxModelFile . empty ()) { nvuffparser :: shutdownProtobufLibrary (); } doInference ( * engine ); engine -> destroy (); return gLogger . reportPass ( sampleTest ); } \u4e3b\u51fd\u6570\u5b8c\u6210\u4ee5\u4e0b\u5de5\u4f5c\uff1a \u89e3\u6790\u53c2\u6570 createEngine doInference \u63a5\u4e0b\u6765\u5177\u4f53\u5206\u6790 createEngine \u548c doInference \u4e24\u4e2a\u51fd\u6570","title":"main \u65b9\u6cd5"},{"location":"tensorrt/trtexec/#createengine","text":"static ICudaEngine * createEngine () { ICudaEngine * engine ; // load directly from serialized engine file if deploy not specified if ( ! gParams . loadEngine . empty ()) { std :: vector < char > trtModelStream ; size_t size { 0 }; std :: ifstream file ( gParams . loadEngine , std :: ios :: binary ); if ( file . good ()) { file . seekg ( 0 , file . end ); size = file . tellg (); file . seekg ( 0 , file . beg ); trtModelStream . resize ( size ); file . read ( trtModelStream . data (), size ); file . close (); } IRuntime * infer = createInferRuntime ( gLogger . getTRTLogger ()); if ( gParams . useDLACore >= 0 ) { infer -> setDLACore ( gParams . useDLACore ); } engine = infer -> deserializeCudaEngine ( trtModelStream . data (), size , nullptr ); gLogInfo << gParams . loadEngine << \" has been successfully loaded.\" << std :: endl ; infer -> destroy (); return engine ; } if (( ! gParams . deployFile . empty ()) || ( ! gParams . uffFile . empty ()) || ( ! gParams . onnxModelFile . empty ())) { if ( ! gParams . uffFile . empty ()) { engine = uffToTRTModel (); } else if ( ! gParams . onnxModelFile . empty ()) { engine = onnxToTRTModel (); } else { engine = caffeToTRTModel (); } if ( ! engine ) { gLogError << \"Engine could not be created\" << std :: endl ; return nullptr ; } if ( ! gParams . saveEngine . empty ()) { std :: ofstream p ( gParams . saveEngine , std :: ios :: binary ); if ( ! p ) { gLogError << \"could not open plan output file\" << std :: endl ; return nullptr ; } IHostMemory * ptr = engine -> serialize (); if ( ptr == nullptr ) { gLogError << \"could not serialize engine.\" << std :: endl ; return nullptr ; } p . write ( reinterpret_cast < const char *> ( ptr -> data ()), ptr -> size ()); ptr -> destroy (); gLogInfo << \"Engine has been successfully saved to \" << gParams . saveEngine << std :: endl ; } return engine ; } // complain about empty deploy file gLogError << \"Deploy file not specified\" << std :: endl ; return nullptr ; } \u5c06 caffe/onnx/uff \u6a21\u578b\u8f6c\u6362\u4e3a TensorRT engine \u5e8f\u5217\u5316 engine","title":"createEngine"},{"location":"tensorrt/trtexec/#doinference","text":"void doInference ( ICudaEngine & engine ) { IExecutionContext * context = engine . createExecutionContext (); // Use an aliasing shared_ptr since we don't want engine to be deleted when bufferManager goes out of scope. std :: shared_ptr < ICudaEngine > emptyPtr {}; std :: shared_ptr < ICudaEngine > aliasPtr ( emptyPtr , & engine ); samplesCommon :: BufferManager bufferManager ( aliasPtr , gParams . batchSize ); std :: vector < void *> buffers = bufferManager . getDeviceBindings (); cudaStream_t stream ; CHECK ( cudaStreamCreate ( & stream )); cudaEvent_t start , end ; unsigned int cudaEventFlags = gParams . useSpinWait ? cudaEventDefault : cudaEventBlockingSync ; CHECK ( cudaEventCreateWithFlags ( & start , cudaEventFlags )); CHECK ( cudaEventCreateWithFlags ( & end , cudaEventFlags )); std :: vector < float > times ( gParams . avgRuns ); for ( int j = 0 ; j < gParams . iterations ; j ++ ) { float totalGpu { 0 }, totalHost { 0 }; // GPU and Host timers for ( int i = 0 ; i < gParams . avgRuns ; i ++ ) { auto tStart = std :: chrono :: high_resolution_clock :: now (); cudaEventRecord ( start , stream ); context -> enqueue ( gParams . batchSize , & buffers [ 0 ], stream , nullptr ); cudaEventRecord ( end , stream ); cudaEventSynchronize ( end ); auto tEnd = std :: chrono :: high_resolution_clock :: now (); totalHost += std :: chrono :: duration < float , std :: milli > ( tEnd - tStart ). count (); float ms ; cudaEventElapsedTime ( & ms , start , end ); times [ i ] = ms ; totalGpu += ms ; } totalGpu /= gParams . avgRuns ; totalHost /= gParams . avgRuns ; gLogInfo << \"Average over \" << gParams . avgRuns << \" runs is \" << totalGpu << \" ms (host walltime is \" << totalHost << \" ms, \" << static_cast < int > ( gParams . pct ) << \"\\% percentile time is \" << percentile ( gParams . pct , times ) << \").\" << std :: endl ; } if ( gParams . dumpOutput ) { bufferManager . copyOutputToHost (); int nbBindings = engine . getNbBindings (); for ( int i = 0 ; i < nbBindings ; i ++ ) { if ( ! engine . bindingIsInput ( i )) { const char * tensorName = engine . getBindingName ( i ); gLogInfo << \"Dumping output tensor \" << tensorName << \":\" << std :: endl ; bufferManager . dumpBuffer ( gLogInfo , tensorName ); } } } cudaStreamDestroy ( stream ); cudaEventDestroy ( start ); cudaEventDestroy ( end ); context -> destroy (); } \u8fed\u4ee3\u591a\u6b21\uff0c\u5206\u522b\u8ba1\u7b97\u5728 GPU \u4e0a\u8fd0\u7b97\u7684\u65f6\u95f4\uff0c\u4ee5\u53ca\u4ece\u5f00\u59cb\u5230\u5b8c\u6210\u8fdb\u884c\u7684 walltime\uff0c\u5305\u62ec\u5176\u4ed6\u8fdb\u7a0b\u4f7f\u7528\u7684\u65f6\u95f4\u548c\u672c\u8fdb\u7a0b\u8017\u8d39\u5728\u963b\u585e\uff08\u5982\u7b49\u5f85I/O\u64cd\u4f5c\u5b8c\u6210\uff09\u4e0a\u7684\u65f6\u95f4\u3002 \u9500\u6bc1\u8d44\u6e90","title":"doInference"}]}